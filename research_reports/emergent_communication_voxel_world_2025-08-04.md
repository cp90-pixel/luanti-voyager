# Emergent Communication in a Multi-Agent Voxel World

## Introduction

Developing AI agents that can **invent and use a shared language** is a long-standing challenge in artificial intelligence and linguistics. By enabling agents to communicate, we allow them to **coordinate as a team** and potentially gain insights into how human language and cooperation emerge. In this research, we design a system of 2–10 agents in a **voxel-based world** (similar to Minecraft) that **start with no common language**. Through interaction and joint tasks (building structures, resource gathering, defending against threats), the agents must **organically develop a communication protocol** from scratch. The focus is on *emergent communication*: the language is not pre-programmed but arises from necessity and feedback in the environment.

*Figure: Example of a multi-agent scenario in a voxel world (Minecraft). Agents (one shown in first-person view on the left) collaborate to achieve tasks within a shared environment (right: a symbolic top-down representation). Such environments enable grounding language in concrete tasks and situations, providing a testbed for emergent communication protocols.*

This system will serve as a testbed to explore how **structured, meaningful language can emerge** among artificial agents. We maintain a general design (not tailoring to a specific domain) while examining crucial aspects of language emergence: how meaning attaches to actions and objects, how efficient the communication is versus how expressive or rich it becomes, and how the "culture" of the agent group evolves and stabilizes their protocol over time. Importantly, we will observe what happens when **new agents join**—whether they can learn the established language or cause it to shift—and we will compare these dynamics to patterns known from **human language development** (e.g. the evolution of grammar or pidgin-to-creole transformations).

## Research Goals

Our research aims to address the following key goals:

1. **Emergence of Compositional and Grounded Language:** Investigate if and how the agents' invented communication becomes **compositional** (i.e. combining simple signals into complex meanings) and **grounded** in their shared world. We expect the language to reflect the environment (e.g. words for landmarks, objects, or actions) and to move beyond one-signal-per-meaning. Prior work has shown that basic compositional languages can emerge in multi-agent systems given the right conditions. We will analyze whether agents develop elements analogous to a vocabulary and simple syntax (for example, combining a "place block" command with a location or color identifier, rather than needing a unique message for every possible action-object pair).

2. **Communication Efficiency vs. Expressiveness:** Analyze the **trade-offs between efficiency and expressiveness** in the emergent protocol. In human language evolution, linguistic structure is thought to arise from a trade-off between making language efficient (compressible) and making it expressive enough to convey nuanced meanings. We will measure how the agents' language balances being **concise** (using minimal signals or shorter messages) with being **clear and expressive** (able to distinguish many states or plans). For instance, an overly efficient code might use very short messages but could become ambiguous or limited (degenerate), whereas an overly expressive code might use lengthy, specific utterances for every situation (inefficient). The goal is to see where the emergent protocol falls on this spectrum and how that relates to task success. We suspect that **pressure for efficiency** (e.g. a cost to sending messages or a limit on vocabulary size) will encourage the agents to develop compositional messages that reuse elements, as seen in other studies, while the need for expressiveness (to achieve high task performance across varied scenarios) prevents the language from collapsing into overly terse or cryptic signals.

3. **Cultural Evolution and Protocol Stabilization:** Observe how the communication protocol **evolves over time** with repeated interactions and how it **stabilizes** into a set of conventions (if it does). We treat the multi-agent interactions as a **cultural evolution process**, where each trial or generation of episodes can modify the "language" slightly. Early on, languages may be unstable or **holistic** (one signal per meaning), but over time we expect conventions to form and possibly **simplify or regularize**. We will examine whether the language reaches an **equilibrium** (a stable shared vocabulary/grammar) or continues to drift. In human studies, it's known that iterated learning and transmission over generations tend to increase linguistic structure and make languages more learnable. Likewise, having a **larger community of agents tends to yield more systematic languages** (more regular grammar and shared conventions). Our experiments will test these phenomena: for example, comparing runs with 2 agents vs. 10 agents to see if larger groups indeed develop more **structured and stable protocols**, and checking if repeated re-playing of the game (simulating generations) leads to increasing regularity. We'll also include measurements of **protocol drift** – how much the language at the end of training differs from the language earlier on – to quantify stabilization. Ideally, after sufficient training, the agents converge to a stable communication system that persists unless perturbed.

4. **Adaptation with New Agents:** Study how the emergent protocol **adapts or accommodates when new agents enter** the population. In practical terms, after the original group of agents has developed a communication scheme, we will introduce a *new agent* (or replace one of the agents with a fresh one that has never learned the protocol) and observe what happens. Key questions include: Does the new agent **learn the existing language** from the others through interaction, and how quickly? Do the established agents **modify their speaking** (e.g. simplify or use more redundancy) to help integrate the newcomer, analogous to how humans might tutor a new speaker? Or does the introduction of new agents cause a **shift or creolization** of the language (where the protocol changes to accommodate everyone)? Successful adaptation would mean the protocol is **general enough** or **systematic enough** that a new learner can pick it up, which is a hallmark of human languages. We will measure things like the new agent's performance over time, and whether communication success rates drop or rebound during the adaptation period. In emergent communication research, one finding is that if agents are trained in a certain way (e.g. with "partitioned" pairings to avoid overfitting to one partner), they can generalize to communicate with *never-before-met agents* using the same language. We will incorporate such strategies (described later) to ensure our protocol isn't just a private code between specific individuals, thereby facilitating new agent adaptation.

5. **Comparison to Human Language Development Patterns:** Throughout the experiments, compare the characteristics of the emergent AI protocol to patterns known from **human language development and evolution**. We are particularly interested in parallels such as:

   * **Compositionality and Grammar:** Do the agents' signals evolve from simple to structured in a way analogous to children moving from one-word utterances to multi-word sentences? We'll check if the emergent protocol shows a **comparable increase in complexity** as the need for expression grows (mirroring how human language becomes more complex as children learn more concepts and as communities grow). As noted, a pressure to communicate a wider range of meanings with limited signals tends to produce compositional grammar in both simulations and human experiments.
   * **Efficiency Principles:** Human languages obey principles like *Zipf's law* (more frequent words are shorter) and other optimizations. We will see if agents develop any analogous patterns (e.g. do they use shorter messages for very common or urgent communications like "enemy spotted"?). This can indicate a convergence toward efficient coding similar to humans.
   * **Cultural Evolution Dynamics:** We'll compare our observed stabilization to how real languages stabilize. For instance, in human history, when communities merge or new members join, languages can simplify or mix (e.g. pidgin languages that later become creoles with more regular grammar). If we see the AI language becoming more regular when a new agent is introduced (because it has to be learnable), that would echo how **iterated learning and new learners** in human groups induce structure in language. We will also look for **common ground negotiation** behaviors analogous to human conversation (e.g. do agents repeat or clarify signals if misunderstood, similar to how human dialogue has repair mechanisms).
   * **Universals and Differences:** Identify any **similarities or differences** between emergent AI languages and natural languages. For example, do the AI protocols show **compositional syntax** (like separate "noun" and "verb" components) or do they rely on holistic signals more than human languages do? Are there signs of **biases** (like certain word orders or preferences) that might reflect cognitive constraints, or are those absent in AI? These comparisons can highlight which aspects of language are likely due to fundamental communication constraints versus those due to specifically human cognitive biases. Recent work has found that without certain pressures, AI agents might develop *different* communication systems than humans, but adding pressures like the **need to be understood, noise in communication, and incremental dialogue** can induce more human-like patterns. Our platform allows us to tweak such conditions and see how the resulting protocols align with human linguistic patterns.

By pursuing these goals, we aim to contribute both to the **engineering of multi-agent systems** (making agents that communicate effectively) and to the **scientific understanding of language emergence** (by drawing analogies to human language evolution).

## Technical Approach

### Grounding Communication in a Shared Voxel Environment

A core principle of our approach is **grounding language in action and perception**. The agents exist in a **shared voxel world** (like a Minecraft realm) where they perceive their surroundings (e.g. seeing blocks, items, other agents or creatures) and can act on the environment (move, place blocks, pick up resources, etc.). All communication takes place within this context, meaning any invented words or symbols must ultimately **refer to something in the environment or the agents' joint task**. We do not provide any predefined vocabulary or meanings; initially, messages are just arbitrary tokens with no built-in semantics. The only way a message gains meaning is through the **effect it has on the recipient's behavior and the resulting task outcome**. For example, if one agent emits a signal and the team's reward improves (because perhaps the signal influenced another agent to do something useful), that signal becomes reinforced as a meaningful "word." There is *no external supervision or dictionary*: the agents form their own concepts relevant to their goals and assign **arbitrary symbols to communicate them**.

To implement this, we integrate communication into the agents' learning loop in the same way as physical actions. Each agent's policy (the decision-making model) outputs not only a physical action (like move north, place block) but also a **communication action** (which could be, for instance, a discrete token or sequence of tokens from some allowed set). All agents operate simultaneously in discrete time steps; at each step they observe the world (which may include any messages other agents broadcast in the previous step) and then choose both an environment action and a message to broadcast. The **messages are part of the environment dynamics** – other agents receive those messages as additional observations. This way, the emergence of a communication protocol is solved jointly with the task: the agents learn **action and communication protocols together via reinforcement learning**. We apply a multi-agent reinforcement learning algorithm (such as policy gradient or Q-learning with shared global rewards) to train the agents to maximize the team's task success. The reward signal comes only from the task outcome (e.g. reward for successfully building the structure or surviving the night) – there is *no direct reward for speaking*. This sparse, task-driven supervision forces communication to arise *out of necessity* for coordination.

**Physically Situated Grounding:** In this voxel world, language is grounded in a rich, sensory environment. Agents might, for instance, see different parts of the world (partial observability), making communication useful for sharing information ("I see a tree at my location"). The world provides concrete referents for potential words: **objects** (like wood, stone, creatures), **locations** (grid coordinates or landmarks), **actions** (building, attacking), and even **agents** (each agent could develop a "name" or identifier). The emergence of language should map onto these elements. Indeed, in prior simulations of grounded language, agents developed symbol sets that mapped to environment features – e.g. distinct symbols for landmarks, actions like "go", and agent identifiers. We expect similar outcomes here: for example, the group might invent a word that consistently means "creeper" (a hostile mob) or a combination of signals that mean "build a wall". Because the environment is *multi-modal* (spatial, visual, temporal), agents can also use **non-verbal cues** in tandem with language. If verbal communication is limited (say by a cooldown or bandwidth), they might use actions as signals (e.g. jumping or hitting a block to draw attention) – akin to pointing or gestures in human communication. Our system allows for these possibilities and treats them as part of the broader communication spectrum (one study observed agents resorting to pointing and guiding behaviors when linguistic channels were unavailable). However, our focus remains on the *explicit messaging protocol* the agents develop via the dedicated communication channel.

We will utilize a platform such as **Project Malmo**, an AI experimentation framework built on Minecraft, or a custom voxel-world simulator. This provides a realistic physics and game environment for the agents. The tasks can be scripted within this world, and agents interact through the Malmo API (perceiving the world state and sending chat messages or other signals). The choice of a voxel/Minecraft-like world is advantageous because it is **open-ended and spatially explicit**: agents can gather materials, build arbitrary structures, and face dynamic threats, which yields a wide range of situations that demand communication. It's also visually grounded, meaning we (as researchers) can interpret what the agents are doing and what their messages might refer to by observing the world (facilitating analysis of the learned language).

### Pragmatic and Task-Driven Communication

We adopt a **pragmatic approach** to emergent communication: agents communicate *only to achieve their goals more effectively*. Every message exchange is in service of the practical tasks at hand. This is in contrast to approaches that try to explicitly encode meaning or grammar rules; here **useful communication is naturally selected** because it leads to higher rewards (successful coordination), whereas pointless or misleading communication fades away. The emergent protocol is thus expected to be highly *pragmatic* – **"say what needs to be said, when it needs to be said"**. For example, if two agents are carrying a heavy resource together, one might develop a signal for "lift now" or "go left" at the critical moment. If an agent spots a danger, a brief alarm signal that causes everyone to prepare for battle would be favored. On the other hand, we would not expect the agents to chat idly about irrelevant things, because there is no incentive (and we might even include a small energy cost for sending a message to reinforce this economy of speech).

In practical terms, we may implement a **communication cost or limit** to encourage efficiency – for instance, limit each agent to a short message (a few symbols) per time step, or subtract a tiny penalty for each message to discourage frivolous communication. This ensures that the **emerging language stays minimalistic and focused**. It also mirrors principles of human linguistic pragmatics where communicators tend to be as informative as needed but not more (*Grice's Maxim of Quantity*). Over time, we expect the agents to hone in on **high-information signals** that directly affect task success, essentially discovering *when* communication is useful (e.g. to share info the other couldn't infer) and *what* information is critical (e.g. location of resources, an intent to perform some subtask, or the presence of an enemy).

We also integrate **contextual grounding**: because all messages are broadcast in the presence of a shared environment, the meaning of a message is tied to context. For example, a single symbol might mean "build" when uttered next to a blueprint of a structure, but mean "attack" when uttered while facing an enemy. Initially, the meaning could be context-dependent in this way (holistic interpretation). Part of our investigation is to see if the agents move towards more **context-independent codes** for clarity (perhaps developing separate words for "build" vs "attack", or combining a generic request word with an observed context to disambiguate). The communication protocol will likely reflect a **balance between explicitness and relying on context**: in very predictable situations, agents might omit obvious details (similar to how humans might just say "the usual place" if context makes it clear), whereas in ambiguous situations they'll include more information. This adaptivity itself is a pragmatic feature we will observe. Notably, earlier experiments found that when only one possible action or target exists, agents may *omit* that from their language (no need for a distinct word), but in varied scenarios they create separate words for each important concept. We anticipate similar results, demonstrating that the emergent language is **grounded and economical**, mentioning things that reduce uncertainty for the team but not over-describing the obvious.

### Protocol Formation and Negotiation Mechanisms

**How do the agents negotiate and agree on a protocol?** In the beginning, with no shared language, coordination will be poor – agents might send random signals or no signals at all, leading to misunderstandings and low task performance. Over time, through trial and error, *successful communication loops* will reinforce certain signal meanings. The process can be seen as a **coordination game** (specifically akin to a **Lewis signaling game** repeated many times until conventions form). We incorporate several mechanisms to facilitate this convergence:

* **Iterative Alignment via Feedback:** The environment's reward acts as an implicit feedback signal. If Agent A sends a message and Agent B responds with an action, they collectively see if the outcome was good or bad. When outcomes are good, the policy updates (via reinforcement learning) will make that message-action pairing more likely in similar future contexts; when outcomes are bad, the agents will tend to try different messages or actions next time. This **reinforcement loop** gradually aligns the agents' understanding of what messages should provoke which responses. Essentially, it is as if the agents are asking and answering *"Did you get that? Let's try again if not."* over many episodes, albeit in a very implicit way through reward signals.

* **Multi-turn Interaction & Repair:** We allow for *multi-turn dialogues* within an episode when needed, rather than forcing a single message exchange. This opens the door to **conversational repair mechanisms**: if one message isn't understood or sufficient, agents can follow up with additional messages. For instance, if Agent A's first signal doesn't lead Agent B to the correct action, Agent A might try a different signal or repeat it more emphatically in subsequent time steps. Likewise, Agent B could have a learned response of sending a **clarification request** (even as simple as a generic "?" signal or staying idle) if it is uncertain, prompting Agent A to elaborate or try again. In natural human conversation, such *repair strategies* (clarifying questions, confirmations, etc.) are crucial for establishing common ground. We aim to see similar phenomena emergently. To encourage this, we might introduce **noise or ambiguity** in communication channels on purpose – e.g. with a small probability a message gets garbled – so that perfect transmission isn't guaranteed. Research shows that when a feedback channel for repair is present and communication is noisy, agents learn to use the feedback to improve understanding, resulting in higher success rates under those difficult conditions. (Interestingly, one study found that having a repair loop made the emergent language *less* strictly compositional, likely because agents relied on back-and-forth clarification instead of encoding everything in one message. We will be mindful of this trade-off: in our design, we want some ability to repair misunderstandings, but we also want to encourage a robust initial message. Balancing these could involve tuning the cost of messages or the urgency of tasks such that there's some—but not too much—room for clarification.)

* **Consensus Building in Groups:** With up to 10 agents, it's possible sub-groups could invent slang or sub-protocols. We implement strategies to drive **global consensus** so that a *single shared language* emerges rather than isolated pairwise codes. One mechanism is **randomized pairing and group compositions during training**: instead of always having the exact same team, we randomly shuffle which agents work together on sub-tasks or split agents into smaller teams that occasionally recombine. This way, an agent must communicate not just with one fixed partner but with **multiple partners across episodes**, encouraging a more general code that all can understand. Recent findings indicate that larger populations *can* develop a shared, aligned language if co-adaptation to one partner is controlled. We may employ the idea of **partitioned training**: temporarily isolate some pairs to establish internal consistency, then periodically mix pairs so they align their conventions globally. Another approach is to periodically have *all agents cooperate together* on a task (ensuring everyone's on the same page) or to have **broadcast communication** (all agents hear each other's messages) so that even if two agents weren't directly interacting before, they still get exposed to the group's vocabulary.

* **Emergent Protocol Negotiation:** We are not hard-coding any explicit negotiation protocol, but we expect to observe natural negotiation in the form of trial and error adjustments. For example, two agents might initially assign different meanings to a symbol; you'll see miscoordination until one agent "gives up" on that symbol or they converge by chance. Over many trials, such conflicts get resolved in favor of one convention (much like how words in human language converge to a single meaning within a community). We'll analyze the logs for evidence of these **dynamics of convergence**: e.g. does one agent's usage slowly dominate others'? Do agents sometimes switch the meaning of a signal mid-training and then all follow the new meaning (indicating a negotiation or repair of a previously poor protocol)? We might visualize this by tracking the frequency of particular message–meaning mappings over time to see them rise or fall, signifying negotiation outcomes.

* **Referential Games as Sub-routines:** As a tool for analysis (and possibly as a side task during training), we can include simple **referential games** between agents: e.g. Agent A sees an object X and has to send a message so that Agent B can pick out X from a set of objects. This kind of mini-game can act as a **language alignment exercise** and is commonly used in emergent language research. We could incorporate occasional rounds of such games (with minimal reward) to help agents fine-tune their vocabulary meanings in a controlled setting, complementing the primary tasks. These would serve as checkpoints to ensure both agents agree on referents (effectively building common ground explicitly). Even if we do not add this explicitly in training, we will use similar **evaluation games** after training to diagnose what each word or message means to each agent.

Through these mechanisms, the protocol should emerge and solidify. The **end result** we anticipate is a set of **conventions**: a mapping from observations/intents to message symbols (for senders) and from heard symbols to responsive actions (for receivers), shared across the agent community. The process of negotiation is entirely emergent, driven by the **need to coordinate** for reward, rather than any hand-crafted language rules.

### Avoiding Private or Degenerate Languages

A potential pitfall in multi-agent communication learning is the development of **"private" languages or degenerate codes** that *do* solve the task but in a way that is not general or interpretable. For example, two agents could develop an arbitrary mapping where specific sensor readings directly correlate to specific signal sequences – essentially overfitting a code to their training environment that no one else could understand (and that might not even make sense if conditions change slightly). Such a code might maximize reward in the short term but fails the spirit of what we consider a "language" (which should be **shareable, extensible, and not overly tied to idiosyncrasies**). We take several steps to prevent this and to encourage the emergence of a **robust, general communication protocol**:

* **Population-Based Learning:** As mentioned, we train not just one fixed pair of agents but a *population* of agents. By having many agents interact in varying combinations, we dilute the possibility of a private code between any two agents. The language must be **mutually intelligible** to the whole group. This is akin to how in human communities, language conventions belong to the community and not just to one pair of individuals. Research supports that training with larger populations (with the right protocol) leads to languages that are more broadly **shared and structured**, rather than fragmented idiolects. We will thus ensure agents periodically rotate partners and tasks, forcing the communication system to be **inclusive and globally coherent**.

* **Alignment Objective:** Inspired by Michel et al. (2022), we can add an explicit objective or regularization term during learning that rewards agents for **aligning with others' communication behavior**. For instance, in addition to task reward, we might compute an **alignment score**: if two agents use the same symbol for the same meaning, that's high alignment; if everyone's signaling diverges, that's low. Implementing this could involve having a "common vocabulary" loss or an imitation component where agents occasionally mimic each other's successful communication strategies. Michel et al. achieved a shared language by partitioning training and then optimizing both communicative success and alignment, resulting in agents that could even communicate with new, previously unseen partners. We will consider a similar **two-fold optimization**: maximize task success *and* maximize a measure of language consistency across the population. This helps guard against multiple incompatible sub-languages.

* **Capacity Constraints:** We will impose limits on the communication channel that incidentally discourage degenerate encoding. For example, limit the vocabulary size (number of distinct symbols) or the message length. If agents had an unlimited channel, they could, in the worst case, encode something like a massive random identifier for every state ("complete memorization"), which is neither compositional nor general. By **capping the vocabulary** and penalizing excessively long messages, we introduce a pressure for **compressibility** – agents are forced to reuse symbols and encode information efficiently. This is known to promote compositional communication: when the number of concepts to convey exceeds the message capacity, agents tend to develop combination strategies (using sequences of a few symbols to distinguish many meanings). Essentially, the agents can't have a unique word for every scenario (a **holistic degenerate code**), so they must find reusable building blocks – which results in more systematic language. We will tune the communication capacity to be sufficient but not overly abundant. For instance, we might allow say 5 distinct symbols and message length up to 3, to see if agents can describe dozens of possible states by combining those symbols (which would indicate compositional encoding).

* **Environmental Variation and Novel Situations:** We design the tasks and environment with significant variation and even *surprise scenarios* to ensure a language with fixed one-to-one mappings won't cope with everything. If a protocol were simply memorizing mappings for a small set of scenarios, it would fail when a new combination arises. So, we introduce **many combinations of tasks and environment states** (different structures to build, different resource layouts, different enemy types, etc.) and even hold out some scenarios to test generalization. The agents thus experience an **expanding meaning space** over time – they are continually pushed to communicate about new things or combinations. This expansion is analogous to a growing culture or a series of novel problems that require the language to stretch. Studies have shown that expanding the meaning space and having multiple partners creates pressure for languages to become more general and compositional. We'll leverage that: for example, once the agents have mastered communicating about task A and task B separately, we might present a scenario where tasks A and B happen together, requiring them to compose their previous signals in a new way. If their language was degenerate (overfit), it will break; if it's robust, they'll handle the novelty. By iterating this process (increasing complexity gradually), we **guide the evolution of the protocol** towards greater expressive power without losing consistency.

* **Regular "Reset" and New Agent Introduction:** Another way to prevent a closed, insular code is to periodically **inject new agents or reset an agent's memory** and see if it can learn from scratch. This simulates **generational turnover**, akin to how human languages are passed to new learners (children) who have limited exposure. Human language evolution models (e.g. Kirby's iterated learning) show that when each generation learns from a subset of the previous generation's language, the language naturally **simplifies and regularizes** to be learnable, avoiding random complexity. We mimic this by occasionally taking an agent that has converged to the protocol and replacing it with a fresh agent that must learn by interacting with the others. If the protocol had drifted into a strange, overly complex form, the new agent would struggle, reducing team performance. The only way to maintain performance is if the language is **relatively simple and learnable** – which will be selected for over these generations. Over time, this should eliminate arbitrary or overly convoluted codes, leading to a more **universal communication protocol** that even newcomers can grasp (which doubles as satisfying our goal of new agent adaptation).

* **Grounding and Transparency:** Because the language is **grounded in the environment**, it is inherently constrained by real meanings. This itself prevents certain degenerate solutions. For example, a truly degenerate emergent language might be something like: Agent A encodes its exact observation (like an image) into a long nonsense string, and Agent B decodes it – effectively a learned encryption. But given our capacity limits and environment variability, such a scheme is unlikely to generalize. Moreover, by analyzing the relationship between messages and world state, we can quickly detect if the agents invented a bizarre encoding (it would look like high mutual information but no apparent structure). To preclude that, we can incorporate a **weak form of pressure for naturalness**: not in the sense of English, but in the sense of **semantic alignment**. For instance, we could augment rewards slightly to favor protocols where the message has high mutual information with *human-relevant* state features. However, simply grounding in the shared environment usually suffices: previous work on avoiding language drift in RL suggests that **perceptual grounding keeps semantics tied to something meaningful**, preventing agents from going off into an uninterpretable drift. In our case, as long as the team needs to reference real objects and coordinate real actions, the language will likely encode those in some consistent way (and if not, our evaluation will catch it, and we can adjust training accordingly).

In summary, through population training, alignment objectives, constrained channels, varied scenarios, and generational testing, we will actively **steer the system away from degenerate communication**. We expect the resulting protocol to be **general, compact, and shared** – much closer to a natural language (in function, though not in form) than to a brittle secret code.

### Quantifying Emergent Language Structure and Efficiency

To scientifically evaluate the emergent communication, we will develop a suite of **metrics** and analysis methods that quantify different aspects of the protocol:

* **Task Performance Metrics:** First and foremost, we measure how well the multi-agent system performs the cooperative tasks *with* communication versus a baseline *without* communication. This includes success rates (e.g. percentage of structures correctly built, resources gathered, survival time against threats) and efficiency of task completion (time taken, resources used). A **positive impact of communication** would be evident if the agents significantly outperform a scenario where they are not allowed to communicate or where they use a scrambled channel. This validates that the language is actually conveying useful information.

* **Communication Utilization:** We will log how often and when agents send messages. This gives a sense of **communication efficiency** – e.g. average number of messages per episode, message length distribution, etc. An efficient protocol might achieve high success with only a few short messages at critical moments. If we see the volume of communication dropping over training even as performance stays high, it indicates the agents are learning to **compress information** better (maybe early on they babbled a lot and later they refined it to a succinct code).

* **Vocabulary Size and Diversity:** Track the number of distinct symbols or signal types that the agents actively use (their **active vocabulary**). Also measure the frequency distribution of those symbols. A well-structured language often shows a **Zipfian distribution** (few symbols used very frequently, many used rarely). We can compare the emergent distribution to human languages in a rough way. Additionally, a *small but adequate* vocabulary suggests the language is efficiently covering meanings without redundancy. If the vocabulary continues to grow without bound as tasks increase, that might mean they are staying holistic (making up new words for new situations) rather than reusing old ones compositionally.

* **Compositionality Metrics:** To gauge compositional structure, we will use standard metrics like **topographic similarity**. Topographic similarity measures how well similarity in meaning corresponds to similarity in messages – formally, we take various pairs of situations and compute a correlation between the distance (difference) in their meanings and the distance between the messages the agents used for them. A high correlation suggests the language encodes similar meanings with similar signals (a sign of systematic structure), whereas a low correlation might indicate arbitrary or holistic encoding. We will also test **generalization**: present the agents with a novel combination of familiar elements (e.g. a new structure made of familiar blocks, or an unseen scenario that mixes goals) and see if they can compose known signals to handle it. If the agents succeed in zero-shot scenarios by combining words (and especially if the combination's meaning is predictable from the parts), that's strong evidence of compositional language. Another metric is **context independence (CI)** – we evaluate if the meaning of a message stays consistent across different contexts. For instance, if a particular signal X means "danger" whenever it's used (regardless of location or which agent said it), that indicates a stable meaning. Prior work suggests using measures like **context independence and topographic similarity** together to assess compositional communication. We will calculate these for the learned protocol. Additionally, we can attempt a more interpretable analysis: mapping symbols to known concepts. For example, do we find one symbol always used when there's a pig and another when there's a zombie? We could train a simple classifier to predict the environment state from the messages and see which aspects of state correlate to which parts of the message, essentially **decoding** the emergent language.

* **Information Content and Efficiency:** Using information theory, we'll compute how much **information is conveyed** by the messages. For example, the **mutual information** between the messages sent and the actual world state (or the intended action) indicates how informative the language is about what's going on. A perfectly efficient language would transmit just enough bits to achieve coordination. We can measure the **entropy of messages** in various conditions: entropy of messages overall vs. entropy of world states. If the ratio is close, they are not over-communicating. We might find, say, that to convey a space of N possible states, the language uses ~log2(N) bits on average – meaning it's encoding information efficiently. Another measure is **redundancy**: do agents sometimes send duplicate or unnecessary signals? If so, that might show up as the mutual information being less than the entropy of messages (some part of messages carries no new info). Redundancy could be a strategy for error-correction (like repeating yourself to ensure understanding), which we can detect if present.

* **Protocol Stability and Drift:** To examine cultural evolution, we'll measure how the communication protocol changes over the course of training (and after introducing new agents). One metric is the **Jensen-Shannon divergence** or similar between the distribution of messages (conditioned on contexts) at different time points. A decreasing divergence plateauing near zero would indicate the language has converged/stabilized. If divergence keeps oscillating or never settles, it means conventions are not solidifying (perhaps due to constant new pressures or instability). We will visualize **language trajectories**: for instance, plotting the meaning of each symbol over time (could be represented by the cluster of states in which it's used). Ideally, those clusters become tighter and more consistent. For new agent introduction tests, we'll see if any **temporary regression** in performance or increase in miscommunication occurs, and how fast it recovers – that gives a measure of **language adaptability**. We might also specifically measure the new agent's **language error rate** (how often it misunderstands or uses a wrong symbol) over time, expecting that to drop as it learns the protocol.

* **Comparative Metrics to Human Language:** If feasible, we will compare some structural metrics of the emergent language to human language benchmarks. For example, measure if an analogue of **Zipf's law** holds for symbol frequency vs rank, or if the distribution of message lengths follows a predictable shape. We can also test the emergent protocol for **combinatorial productivity**: can it express exponentially many meanings with linear growth in symbols? (Human language can, because of combinatorial grammar). We might design a **productivity test**: have agents communicate about an increasingly large set of object combinations and see if they just invent new words (which would scale poorly) or reuse finite words in combinations (scaling efficiently). This ties back to compositionality and expressiveness.

* **Human Interpretability (optional analysis):** While not required for the agents' success, we are curious whether the emergent protocol has structures that a human can interpret or that resemble known languages. We will create **visualizations** of the learned language: for example, a **dictionary** mapping each learned symbol (or sequence) to the situations it's used in most. We could present these to human evaluators or just qualitatively assess if they make intuitive sense (e.g. maybe the symbol "^%$" always appears when there's an enemy; we could label it an "alarm" word). Another approach is to see if we can **map the emergent words to English** by correlating them with known labels (if our world is like Minecraft, we know what concepts exist, so if a certain emergent token correlates strongly with, say, the presence of a "pig", we can align that token with the word "pig"). The degree to which a mapping exists is interesting. If the language is highly efficient but alien (no one-to-one mapping, perhaps very distributed encoding), mapping will be hard. If it's more structured (e.g. one token per concept), mapping will be easier. This isn't to judge the language as "good" or "bad", but to understand its structure. We may employ embedding techniques: represent each symbol by the contexts in which it appears and cluster them, to see if the clusters correspond to intuitive categories (like all location-indicating symbols cluster together, etc.).

These metrics will be reported for each experiment and used to support conclusions about our research questions. For example, to address goal (1) on compositionality, we'll specifically look at topographic similarity and generalization tests. For goal (2) on efficiency vs expressiveness, we'll examine information content, vocabulary size vs concepts encoded, etc., demonstrating the trade-off. For stabilization (goal 3), we present drift metrics and show convergence. For new agents (goal 4), we present adaptation curves and maybe a drop-then-recovery in success rate when a new agent is added. For human comparison (goal 5), we highlight any human-like patterns found (or note differences like if the agents' language violates something humans usually obey).

## System Design

### Multi-Agent Learning Framework

Our system uses a **multi-agent reinforcement learning (MARL)** framework where each agent is a deep neural network-based controller. All agents share the same training algorithm, but they do not necessarily share weights (we can consider both **homogeneous agents** that learn from each other's experiences or **heterogeneous** with separate networks – likely we'll use a homogeneous policy for simplicity and let agents differentiate by observation). At each time step, an agent receives its **observations**: this includes its local view of the voxel world (e.g. a grid of nearby blocks, its own status like health/inventory, any visual or ray-cast perceptions of the environment) as well as **messages from other agents** (e.g. a list of tokens that were broadcast by others on the previous tick). The agent's policy network processes these inputs to output two things: (1) a **next action** (from the set of environment actions like move, turn, use item, attack, etc.), and (2) a **message to send** (which could be a discrete token or a sequence of a few tokens chosen from the allowed vocabulary).

To handle sequential decision-making and communication, each agent's policy can be a **recurrent neural network** (RNN) or have memory (like an LSTM or a transformer with history). This allows the agent to maintain an internal state (e.g. remembering what it's currently trying to do, or what was communicated moments ago) which is important for coherent communication (e.g. to carry on a "dialogue" or follow a plan). The communication channel itself we design as follows: at every step each agent can emit at most one symbol from a vocabulary (say vocabulary of size *V*). If we allow sequences, the agent could output up to *K* symbols in one turn (which could simply be modeled by having *K* time steps of communication for one action step, or more simply, treat each message as a fixed-length vector of *K* slots where it can place symbols or a null token). For training, discrete symbols cause non-differentiability, so we will use techniques like the **Gumbel-Softmax reparameterization** or policy gradient methods to allow learning of communication. Mordatch & Abbeel demonstrated that one can even backpropagate through discrete communication by making the whole process differentiable; we may follow a similar approach if feasible (e.g. treating communication decisions as continuous during training and sampling during execution). Alternatively, a simpler MARL approach is to have a joint reward and use an actor-critic that handles each agent (possibly with a centralized critic that sees all agents' observations and messages during training, to stabilize learning). Our system likely uses a **centralized learning, decentralized execution** paradigm: during training a critic network gets to observe the global state and all communications to assign credit properly, but during execution each agent only has local observations and incoming messages.

**Learning Algorithm:** We will use state-of-the-art MARL algorithms like **MADDPG (Multi-Agent DDPG)** or **QMIX** or **PPO** adapted for multi-agent, depending on what fits the continuous/discrete mix of actions. The key is that the reward is **shared** among agents (fully cooperative game), so we avoid agents getting selfish signals – they all succeed or fail together, which encourages genuine cooperation and communication. We might introduce *shaping rewards* for intermediate progress (like partial reward for collecting some resources) to aid learning since pure sparse reward at task completion can be hard. But we must be careful that any shaping doesn't inadvertently give away communication meaning (we keep it task-oriented). The training will iterate over many episodes in the voxel world (each episode could be e.g. a day in Minecraft, within which they must build a shelter before night).

Because the agents learn simultaneously, we have to address **non-stationarity** (one agent's policy changes while the other's is changing). Techniques like experience replay are tricky in MARL for this reason, but using a centralized critic or sharing policies can help. We will likely train using *self-play* style: initialize agents with random behavior and let them co-evolve their policy and language. Over millions of steps, they should hopefully converge to a coordinated policy (this requires a lot of training, but we can leverage parallel environment instances to collect more experience, a common practice in training agents in complex games). Additionally, we may curriculum learning: start with simpler tasks or fewer agents (to bootstrap some basic communication) then progressively increase complexity or number of agents.

**Communication Protocol Representation:** Internally, each agent will have to **encode and decode messages**. For example, the agent's neural network might have an output head that produces a probability distribution over possible messages (or sequences). The receiving agent could have an input layer that is essentially an embedding of the received symbol (similar to how words are embedded in NLP models). If multiple agents speak at once, we have a design choice: either allow **simultaneous broadcast** (each agent's message is out there and a receiver might get a set of messages in one tick), or enforce a simple turn-taking (e.g. agents take turns or only one can speak at a time per tick). A compromise: they can speak simultaneously, and the receiver's input is a concatenation or multiset of symbols. In practice, we might limit channel bandwidth to one message globally per time step to avoid too much overlap (or have a random ordering where each tick only a subset of agents can speak). This ensures messages don't drown each other out. However, part of the challenge could be *learning to manage the communication medium*, which is also interesting (like figuring out not to all talk at once, perhaps implicitly by negative reward if messages collide or by making simultaneous messages less effective). For now, we could assume a controlled setting where at most one or few messages are sent at once.

During training, **exploration** is critical. Early random messages are likely useless; we may need to encourage exploration of the communication space. We can do this by adding entropy regularization on the message policy (to try a variety of symbols) and by giving agents diverse experiences (so they see reason to try communicating). We might even pre-train or warm-up by giving them a very small amount of supervised experience in a trivial signaling game (like one agent sees a goal and must send a signal to another agent to go there) just to seed the idea that using the channel at all can be useful. But this pre-training would use *arbitrary symbols* – we wouldn't bias it toward any particular language, just demonstrate that using the channel at all can be useful. After that, we let the complex language emerge freely in the full tasks.

### Environment and Task Scenarios

We design a **suite of tasks in the voxel world** that require tight coordination and thus incentivize communication. The tasks are inspired by typical Minecraft objectives and are made cooperative so that no single agent can easily do it alone (ensuring they *need* to talk and work together):

* **Building Structures:** The agents are tasked with constructing a specific structure (for example, a small house, a bridge, or a tower). The blueprint or goal structure can be given abstractly (like an input that each agent receives, or only one agent knows the blueprint and must communicate it to others). Agents need to gather materials and place blocks in the correct configuration. This requires **division of labor** (perhaps one agent fetches wood, another lays blocks) and **spatial coordination** ("build the base here, then walls", etc.). Communication is useful for negotiating roles ("you get wood, I'll get stone"), synchronizing actions ("place your block after I place mine"), and for joint navigation ("come to coordinates X"). Since the structure may be complex, the language might need to reference positions (maybe relative directions or a shared reference frame) and object types. We'll randomize aspects like the structure shape or the environment layout so they can't solve it by rote; they must truly communicate plans.

* **Resource Gathering and Crafting:** In a survival scenario, agents must collect various resources (wood, ore, food) scattered around and perhaps **craft** them into tools or items. Items might be located far apart or behind obstacles such that no single agent perceives everything. Communication helps them **share knowledge** ("I found iron here") and **coordinate meet-ups or exchanges** ("let's meet at the base to combine resources"). We can introduce **limited inventory** so no one agent can carry everything, forcing trade and coordination. This scenario tests the emergent language's ability to refer to resource types, quantities, locations, and to plan sequences of actions (e.g. first do A then B). It also might spur the development of temporal terms or signals like "done" or "need help".

* **Defending Against Threats:** We introduce dynamic threats such as hostile mobs (zombies, creepers) attacking the group or a base they must defend. This adds a **time-critical, adversarial element**. Communication becomes crucial for **warnings** ("enemy coming from east"), **strategic planning** ("attack now", "fall back"), and perhaps **assigning targets**. Because threats cause stress on the agents, we expect possibly very **efficient, alarm-like signals** to emerge (similar to alarm calls in animal communication, which are short and urgent). This scenario also ensures that communication protocol is tested under duress – the agents might develop a quick shorthand for emergencies. We'll vary enemy types and attack patterns to require general signals (not one per exact event).

* **Exploration and Navigation:** A task where agents must explore a large area to find a goal (e.g. locate a hidden treasure or a goal location) and then converge there. Each agent only sees a small area, so they must communicate discoveries ("I see the beacon") and possibly guide each other ("go north 10 steps from me"). This will test their ability to communicate **spatial information** and **directions**. If the environment is procedurally generated, they can't rely on static positions – they must describe relative positions or landmarks. We might see the emergence of a rudimentary **map-like language** (like "X" for water, "Y" for mountain, etc., and ways to say near/far).

* **Heterogeneous Roles Scenario:** We can assign slight differences to agents (e.g. one agent can mine stone faster, another can craft, another can fight well). This asymmetry forces them to **coordinate roles** because each has a specialty. The communication might then include **requests** ("fighter come here", "crafter make this item now"). This tests if the language can incorporate the notion of agent identity or roles. Indeed, in prior work agents spontaneously developed symbols to refer to specific other agents when needed. We expect if roles are salient, some of the emergent vocabulary will correspond to "you" or "me" or agent identifiers, especially in instructions ("Alice, do X.").

We will run these scenarios in combinations and in sequence to push the language's richness. Initially, we might train on one task at a time to see if a coherent protocol emerges per task. Later, we can combine tasks or switch tasks to see if the language is *general* (the same language being extended) or if the agents struggle (indicating the need for more training or adjustments).

The voxel world will simulate physics like Minecraft: agents have to actually traverse terrain, spend time to mine or build, etc. This means communication not only has to convey *what* to do, but sometimes *when* and *in what order*. For example, if two agents carry a heavy object, one might have to count or signal timing to lift together. The continuous nature of time encourages possibly the development of sequential signals ("3, 2, 1, lift!") or at least a ready/acknowledge protocol. We will watch for emergent patterns such as **turn-taking in communication** (do they learn to alternate who speaks when, to avoid confusion?), and **temporal signals**.

**Reproducibility and Experimental Control:** We will implement the environment in a way that experiments are deterministic or with controllable random seeds, so that we can repeat scenarios and isolate the effects of various design choices. The **experimental setup** (the code, environment configuration, and agent algorithms) will be made fully reproducible. This ensures that other researchers can run the same multi-agent voxel world experiments and verify or build on our results.

### Training Regime and Cultural Evolution

We structure the training in phases to encourage the gradual emergence and testing of the communication protocol:

* **Phase 1: Basic Coordination Learning:** Initially, run many episodes with a small number of agents (say 2 or 3) on simpler versions of tasks. The goal here is to get the basics of coordination and some rudimentary signals established. For instance, in a simple gathering task, they might learn that a certain peep means "I found something". We allow plenty of exploration and do not enforce too many constraints yet, to let them find any successful strategy.

* **Phase 2: Scale Up Population and Complexity:** Once basic communication is present, we increase the number of agents and introduce more complex tasks or multiple tasks. Now the challenge is for them to **extend and refine their language** to handle more agents and scenarios. At this phase, we start applying the **population mixing** training: shuffle agents among teams for different episodes, ensuring the protocol stays unified. We also start measuring compositionality and efficiency metrics here to verify that as the scenario grows, the language adapts (e.g. do they start composing signals or just invent many new ones? Ideally, we see more reuse/composition).

* **Phase 3: Introduce Perturbations (Cultural Evolution tests):** Here we simulate long-term evolution. For example, after the protocol seems stable, we **swap out an agent** with a fresh one (new agent has no prior training in language). We then continue training (or just observe if we freeze others and train the new one) for some episodes. We monitor how quickly the new agent picks up the language and whether the existing agents adjust their signals to accommodate the newcomer. We might repeat this multiple times (like successive generations of new agents entering). Another perturbation is to introduce a **temporary split**: divide the population into two sub-populations that train separately for a while (possibly developing dialects), then bring them back together and see if they can reconcile the differences. This is analogous to communities diverging and then meeting – a test of how **robust and flexible** the language is. We expect if the protocols diverged, there will be an adjustment period with lower performance until they align again, potentially with one convention winning out or a merged protocol forming. This helps study how *cultural merging* might work in AI languages, and parallels human language contact situations.

* **Phase 4: Stress Testing and Generalization:** Finally, we test the fully trained agents on **unseen scenarios or extreme conditions**. For instance, a very large map, or a need to do two tasks at once, or work with the maximum of 10 agents if they usually trained with fewer. We disable further learning and just see if the protocol and teamwork hold up (generalization). This phase evaluates how well the learned communication scales and whether it truly grasped general principles. We'll also test resilience: what if there's noise in communication or one agent's messages get lost occasionally (if not already tested)? Do they have redundant strategies or fallbacks (like switching to non-verbal cues)? These stress tests solidify our claims about the emergent protocol's capabilities and limits.

Throughout all phases, we maintain logging for analysis: chat logs, actions, rewards, etc., to retrospectively analyze how the language evolved at each stage. We will likely produce **animations or transcripts** showing how in early training episodes communication was chaotic, but later episodes show clear consistent messaging corresponding to events.

Importantly, we ensure that **broad generality** is maintained. We are not building a specialized protocol for only one narrow task; rather, the environment is rich enough and our training varied enough that the resulting communication protocol should be generally useful for "agents living in a voxel world doing common survival tasks." Our design avoids single-purpose shortcuts, pushing for flexibility.

### Adaptation to New Agents and Ad-Hoc Teams

When a new agent (with no prior language exposure) enters a group of trained agents, we have a few strategies to handle this scenario:

* **Learning by Immersion:** The new agent will start with random policy and will be quickly immersed in interactions where others are using the established protocol. We allow the new agent to learn (via reinforcement) from scratch in these interactions. Because the rest of the team already performs well (using the language), the pressure is on the new agent to adapt – if it doesn't understand messages, its actions will likely lead to lower team reward, creating a gradient for it to adjust its policy to respond correctly to those messages. Similarly, when the new agent tries to send messages, initially they might be nonsense, but if those hurt performance, the agent will learn to adjust its messaging to elicit better responses (or possibly the others might occasionally guess/interpret the new agent's attempt and reinforce it). We will see how fast this agent catches up. **Success criteria**: the new agent's performance converges towards that of the experienced agents and the overall team performance returns to near its original level after some episodes. This would demonstrate the language is learnable from context and reinforcement, much like a child picking up a language by being surrounded by speakers.

* **Social Learning Aids:** We can optionally enhance adaptation by allowing some **teaching behavior** from experienced agents. For example, if the new agent is not responding correctly, experienced agents might initially adjust by using more **demonstrative actions or redundant messaging**. We could encourage this by slightly modifying the reward for experienced agents when a newcomer is present, to penalize if the team fails (so they have incentive to help the newbie succeed). In response, we might observe behaviors like an agent sticking close to the new agent, showing it what to do while using a certain signal – effectively grounding the signal for the new agent. This would be a fascinating emergence of a kind of **scaffolding**, similar to how humans use simpler language (or gestures) to teach newcomers. While we don't hard-code this, the environment might implicitly create it (since if new agent fails, everyone loses reward, so others might naturally adjust to mitigate that).

* **Ad-hoc Team Communication:** Another test is mixing two groups of agents that were trained separately (possibly with slight differences in their emergent languages) and seeing if they can **bridge the communication gap**. This is analogous to two dialect speakers trying to collaborate. We expect initial misunderstandings, but they might converge to a common protocol after some interaction. We could facilitate this by giving a bit of additional learning time or allowing them to run a simple coordination exercise first to calibrate. If our alignment mechanisms (population training, etc.) were strong, it's possible both groups learned a similar language anyway (especially if they had common constraints), but if not, we'll analyze how quickly and in what manner a consensus arises. This addresses **protocol negotiation on-the-fly** and the resilience of the language conventions when encountering unfamiliar partners.

We will document the adaptation process: e.g., how many episodes did it take until a new agent's action choices given a message align with what older agents expect, or until the messages the new agent sends start matching the established vocabulary usage. If the language is truly systematic, the new agent might, for instance, independently discover that when it hears "token1 token2" from others it should perform the composite action corresponding to those tokens (because that yields reward). If we see that after learning, the new agent uses the same tokens in the correct order as others do, it means the protocol was **transparent enough to learn from scratch** (a very positive sign). In contrast, if the protocol had been a very opaque code, the new agent might never figure it out without explicit supervision, which would be a failure of our design in terms of generality. Based on prior research, we expect our measures (like population training and limited co-adaptation) will yield a language that *is* usable by new agents.

### Human-Interpretability and Scaffolding (Optional Layer)

While our system is aimed at **emergent, agent-centric communication**, we have the option to add a layer of **human-interpretable scaffolding** if needed or for analysis. This means designing the system such that we (or a human user) can get partial insight or even intervene in the communication. Some strategies include:

* **Transparent Vocabulary Interface:** We could assign each abstract symbol a unique ID and at the end of training attempt to label them with human words for our understanding. For example, if symbol #3 seems to always be used when there's an enemy, we label it "enemy". This doesn't affect the agents (they operate with symbol #3 internally), but it provides a *dictionary* for us. If we wanted to gently enforce interpretability, we might initialize the symbols with some **human mnemonic tags** (like using emoji or letters as initial tokens) – however, we must be careful not to bias the emergence. It may be better to post-hoc translate rather than predefine them.

* **Human-in-the-loop episodes:** Optionally, after the agents develop their protocol, we could insert a human player controlling an agent in the environment to see if we can learn their language or if they can learn ours. This isn't a primary goal, but it's a fascinating extension: if the emergent language has some intuitive grounding (like maybe an agent waves or jumps as a sign, which a human might guess), there could be some basic communication. We could also try to have a human demonstrate a task using a limited set of signals and see if agents pick up those signals (scaffolding the language with a bit of human guidance). However, since the prompt suggests keeping scaffolding optional, we will not rely on human data for the main training, only as a possible overlay for interpretability.

* **Layered Protocols:** We might design a two-layer communication system where one layer is the emergent protocol, and another layer (optional) is a translation to human language. For example, train a separate neural translator that observes the world and the agents' messages and tries to output an English description of what the message likely means. This can be done by collecting a dataset of contexts and emergent messages and labeling it (via our own understanding or a heuristic) to train a translator. The translator doesn't affect agent behavior; it's purely an analysis tool to produce a human-readable **gloss** of the agent language. Such a tool is part of our **analytical deliverables** – helping researchers and users understand the otherwise cryptic emergent code.

* **Preventing Semantic Drift for Interpretability:** If we had a scenario where we wanted agents to eventually interface with humans, we might include a small bias toward words that align with human words (e.g. reward them if one of their signals coincidentally matches a known concept consistently). This is outside our main scope (since we don't require human understanding for the agents to succeed), but it's an optional layer where we check if, for instance, we can retrofit the emergent language into a subset of English commands. Some research suggests adding a natural language model constraint helps keep communication more human-like, but in our case, we focus on emergent protocols first, then consider alignment to human language as a separate step if desired.

In all, the system is primarily designed to let agents develop *their own* communication from scratch. The human-interpretable scaffolding is a bonus layer to either guide the emergence slightly or to decode it after the fact. We will likely keep it "off" during the main training (to not interfere with natural emergence) and only apply decoding and visualization tools afterward. The data collected (message logs aligned with world states) is extremely valuable for linguists or AI researchers to analyze patterns – this forms part of our deliverables.

## Deliverables

By the end of this project, we will produce:

* **Algorithmic Framework for Emergent Communication:** A detailed design and implementation of the multi-agent learning algorithm that enables emergent communication under sparse, task-oriented rewards. This includes the RL training code, the architecture of agents (observation and action spaces with communication), and any specialized techniques (e.g. differentiable communication module, alignment loss, etc.) that we developed to facilitate language emergence. The framework will be documented so that others can apply it to similar problems. For example, we will provide pseudo-code or actual code for the training loop that interweaves agent actions and communications, showing how reward and learning updates propagate in this setting.

* **Metrics and Evaluation Suite:** A set of metrics (as described above) and analysis scripts to evaluate language quality, efficiency, compositionality, and stability. We will deliver both quantitative metrics (like topographic similarity scores, message entropy, success rates) and qualitative analysis tools (like visualization of communication sequences, clustering of meanings). These will be packaged in an **evaluation toolkit** that can take the logs from an experiment and output graphs and tables: e.g., a graph of task success vs. number of messages used, a timeline of how a particular symbol's usage frequency changed over training, or a confusion matrix of meanings vs. messages. The goal is to make evaluation of emergent languages systematic and reproducible.

* **Reproducible Multi-Agent Voxel World Environment:** The complete experimental setup in a Minecraft-like voxel world, including all the task scenarios, environment configurations, and agent definitions. We will provide either the configuration for Project Malmo or an equivalent custom environment code. This deliverable ensures that anyone can replay our experiments or modify the tasks. It will include documentation of the **task protocols** (what the agents are supposed to do in each scenario) and any specific settings (world size, number of resources, etc.). Essentially, it is a **testbed** for emergent communication research, built around a voxel world theme. Along with this, we'll supply some example runs or video recordings demonstrating agent behaviors and interactions in the world, to illustrate qualitatively how they cooperate and communicate.

* **Analytical Tools for Protocol Decoding and Visualization:** We will deliver tools (and results) for interpreting the emergent communication protocols. This includes the learned dictionary or mapping of signals to meanings (as far as we can determine it), visualizations like graph networks of symbols (linking symbols that often occur together or in similar contexts), and possibly a trained "translator" model that tries to convert agent messages into human-readable descriptions. We will also provide **visualizations of agent trajectories with communication**, e.g. replaying an episode and annotating it with the messages exchanged at each step, which is extremely useful for understanding how the language works in practice. These tools not only help us analyze the results but are deliverables for others who want to examine or present the phenomena of emergent language. For example, we might include Jupyter notebooks that load the log data and automatically highlight when a certain token is used and what happened next, helping to infer its meaning.

* **Scaffolding and Human-Interaction Strategies (Optional Module):** If explored, we will describe and provide any strategies we tested for integrating human-interpretable signals or for an agent to interact with humans. This could be in the form of a short report or code that allows a human to send messages to agents and vice versa using the learned protocol (with translation). Even if we do not fully implement this in the main experiments, we outline a **layered approach** where a human-friendly command set could overlay the emergent language for practical applications. For example, we might document how one could map the emergent protocol to a set of English commands if one wanted to use these agents in a human collaboration setting. This deliverable is more **conceptual** if not fully implemented, but it ensures we keep in mind the pathway to making the system accessible beyond just AI-to-AI communication.

All deliverables will be made available in a coherent manner, likely as a research paper/report accompanied by code and supplementary material. The **system design** is kept general so it can be adapted to other environments (not just voxel worlds) and the insights are broadly applicable to emergent communication research. We will validate the system through extensive experiments and include those results in the deliverables to demonstrate how each component of our design influenced the outcomes (e.g. ablation studies showing if we remove the vocabulary limit or the population mixing, what happens to the language).

## Conclusion

In this project, we proposed a comprehensive system for studying **emergent communication among AI agents in a rich, game-like environment**. By having 2–10 agents collaborate in a voxel world with no initial language, we delve into how communication can originate and evolve purely from interactive necessity. Our design emphasizes real-world grounding (tying language to actions and perceptions in the environment) and draws inspiration from both **linguistic theory** (e.g. compositionality from pressure, cultural evolution) and **multi-agent learning advances** (population training, differentiable communication).

We expect to see the agents develop a **shared protocol** that, while not pre-programmed, exhibits many properties analogous to natural languages: it should become **compositional** (to efficiently handle many scenarios), **efficient** yet **expressive** (balancing brevity with informativeness), and **stable** as a convention within the group. By monitoring the system over time and under various perturbations, we can observe a form of **culture**: the language may undergo periods of rapid change, then stabilization, and adapt when facing new circumstances (like new group members), much as human communication systems do.

Crucially, this research sheds light on the open questions of how **compositional and grounded languages emerge** – offering empirical evidence for theories that such structure arises from the dual pressures of needing to compress information and needing to be understood across a community. It also informs the engineering of multi-agent systems: the insights gained (e.g. how to encourage a global language and avoid private codes) can be applied to design AI that can **coordinate with each other (and potentially with humans) in flexible ways**. By comparing the emergent protocols with human language patterns, we deepen our understanding of which aspects of language are general solutions to communication problems and which might be tied to human-specific factors.

In summary, the outcome of this project is not just a single trained multi-agent system, but a **framework and set of principles** for emergent communication in embodied AI. We deliver a validated example of that framework in a voxel world setting, along with metrics and tools for others to explore this fascinating intersection of AI, communication, and collective behavior. This work moves us closer to AI agents that can **invent languages as needed** to cooperate in new situations, an ability that could be crucial for complex deployments of AI in the real world, and simultaneously provides a sandbox to test theories of language evolution in a controlled, repeatable manner.

**Sources:**

* Igor Mordatch and Pieter Abbeel. *"Emergence of Grounded Compositional Language in Multi-Agent Populations."* (2018) – Demonstrated that agents can develop a basic compositional language of discrete symbols grounded in a physical environment, especially when pressured to keep vocabulary small and face varied tasks.
* Paul Michel et al. *"Revisiting Populations in Multi-Agent Communication."* NeurIPS 2022 – Found that careful training with populations yields a shared language; larger groups developed more structured, compositional languages and agents could even communicate with new partners not seen during training.
* Mitja Nikolaus. *"Emergent Communication with Conversational Repair."* ICLR 2024 – Explored the role of feedback and clarification in emergent communication, showing that a repair mechanism improves robustness to noise and mimics human common-ground building (though it may reduce compositionality).
* Simon Kirby et al. (2015) – Research on cultural language evolution demonstrating that linguistic structure (grammar) arises from a **trade-off between compressibility and expressivity** in iterative learning scenarios. Limor Raviv et al. (2019) – Showed that even without generational turnover, **larger communities and more varied meanings lead to more systematic, compositional languages**, aligning with our approach of expanding tasks and group size to induce structured communication.
* Microsoft Research Project Malmo – Provided a Minecraft-based platform for multi-agent collaboration research, which we leverage for creating a complex, physically grounded environment to study emergent communication in a setting approachable to human-realistic scenarios.