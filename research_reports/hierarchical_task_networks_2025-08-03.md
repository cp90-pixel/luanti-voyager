# Hierarchical Task Networks for LLM-Based Minecraft Agents

*Deep Research Report - Generated by OpenAI Deep Research*  
*Date: 2025-08-03*  
*Prompt: #3 - Hierarchical Task Networks for Voxel Worlds*

## Context and Challenges in an Open-World Environment

Implementing a Hierarchical Task Network (HTN) in a Minecraft-like sandbox world poses unique challenges. The agent operates in an **open-ended voxel environment** where goals like "build a house" require multi-step plans involving building, crafting, and exploration. **Large Language Models (LLMs)** can provide high-level reasoning and commonsense knowledge, but we need a structured planning approach to break down goals and react to the dynamic world state. The design must allow the agent to **decompose complex goals into actionable steps**, handle unexpected events (like sudden monster attacks or nightfall), manage resources (materials, health, time of day), and **learn** from successes and failures over time.

In such an environment, a purely end-to-end approach (e.g. reinforcement learning mapping goals directly to low-level actions) is inefficient for long-horizon tasks. Instead, a hierarchical planner can leverage the LLM's reasoning to **bridge high-level goals and low-level game actions**. The **HTN-based design** uses the LLM to decompose goals into subgoals and actions, providing structure and adaptability. This was demonstrated in recent work like *Ghost in the Minecraft (GITM)*, where a hierarchical agent integrated LLM planning with a knowledge base and memory to achieve much higher success on tasks like obtaining rare items.

## Representing HTN Tasks for LLM Reasoning

**How should we represent tasks and sub-tasks so that an LLM can reason about them?** A practical approach is to use **natural language descriptions or a light-weight structured format** that the LLM can easily interpret. Each HTN node (task) can be represented as a small template including:

* A **Task Name or Goal** (e.g. *"Build a furnished house"*).
* A description of **preconditions** (if any) and **context** (e.g. "requires a safe, flat area; it's currently daytime").
* A list of **subtasks** or methods to achieve the goal.

Because LLMs excel at understanding and generating text, tasks can be described in semi-structured English. For example, you might represent a task and its decomposition as follows:

```plaintext
Task: BuildFurnishedHouse(location)  
- Ensure location is safe and clear of mobs.  
- Gather necessary materials (wood, stone, glass, wool).  
- Construct the house structure (floor, walls, roof, door).  
- Craft furniture items (bed, chest, crafting table, furnace, torches).  
- Furnish the interior with the crafted items.  
```

Each sub-bullet represents a subtask that can further be decomposed. This kind of representation is human-readable and LLM-friendly – the model can be prompted with these task outlines and asked to elaborate, monitor conditions, or suggest alternatives. In practice, *Ghost in the Minecraft* used **"structured actions" described in text** to interface the LLM with game controls. By defining a set of textual action descriptions (like "walk to X", "craft Y", "attack Z"), the HTN planner can output a sequence of actions that the LLM understands, while a lower-level controller maps them to game API calls or keyboard inputs. The textual representation **bridges the gap** between high-level intent and low-level execution, leveraging the LLM's strength in language understanding.

To make HTN nodes even more LLM-accessible, you can encode them in simple JSON or pseudo-code that the model has been trained to parse. For example, an HTN method could be represented as:

```json
{
  "task": "BuildHouse",
  "preconditions": ["have ShelterSite"],
  "subtasks": [
    "Gather(Materials)",
    "Construct(Structure)",
    "Craft(Furniture)",
    "DecorateAndFurnish()"
  ]
}
```

An LLM can be prompted with such a structure and asked to fill in details (e.g., what materials are needed given the current inventory and environment). The key is to keep the format **understandable and consistent**, so the LLM can act like a reasoning engine over the tasks. Some frameworks even translate tasks to formal planning languages (like PDDL or HDDL) behind the scenes, but in an LLM-centric approach it often suffices to use plain language with a clear format. The LLM's world knowledge can then be applied – for instance, knowing that "glass" comes from smelting sand, it might automatically include a subtask to gather sand if the plan requires windows.

**Dynamic HTN node representation:** Each task description can be made dynamic by including state variables or conditions that the agent will fill in at runtime. For example, \*"Gather(Material)\*\*" could be expanded to *"Gather(wood, quantity=20)"* based on what the house size is and how much wood is already in inventory. By deferring certain decisions and letting the LLM reason about them on the fly, the HTN remains flexible. The agent could query the LLM with the current state (inventory counts, time of day, etc.) to decide parameters for tasks (e.g. how many logs to chop, whether to build a wooden or stone house depending on available resources). This approach treats the LLM as a **procedural knowledge base** that can fill in task details and even suggest new subtasks when needed.

## Dynamic and Resource-Aware Task Decomposition

In an open-world sandbox, **task decomposition must continuously adapt to the world state**. This means the HTN planner can't be static – it should take into account the agent's current context (resources, location, threats, time) at each decision point. Several strategies ensure dynamic, resource-aware planning:

* **Incorporate Preconditions and Context Checks:** Each HTN method can specify conditions under which it's applicable. For example, a method for "BuildHouse" might have a precondition that the agent has at least 20 wood planks; if not, the planner should first insert a subtask to collect wood. The LLM can be prompted to consider such conditions ("If inventory wood < required, then subtask = GatherWood"). By encoding resource checks into the task descriptions, the plan naturally becomes resource-aware. This prevents the agent from attempting actions it can't complete (like trying to build a roof without enough wood) – instead, it will plan to fulfill those requirements first.

* **On-the-Fly Subgoal Generation:** The LLM's reasoning ability is useful for dynamically generating subgoals when unexpected needs arise. For instance, while executing a plan to build a house, the agent might realize it needs a **crafting table** to craft a door. A dynamic HTN system could consult the LLM at that moment to insert a new subgoal ("craft a Crafting Table") into the plan. Because the LLM knows the **prerequisites** for crafting (wood for crafting table, furnace for smelting, etc.), it can augment the HTN with additional steps that a static planner might miss. This resembles *Goal-Driven Autonomy*, where the agent can form new goals in response to the environment.

* **Adaptive Method Selection:** HTNs often allow multiple methods to achieve the same task. A dynamic HTN planner can choose methods based on the current world state. For example, for the task *"AcquireLightSource"*, one method might be *"craft torches if coal and sticks are available"* and another might be *"craft charcoal if no coal"*. The planner (or the LLM guiding it) should pick the method that matches the agent's situation (do we have coal? is it daytime to smelt charcoal safely?). By training or prompting the LLM with knowledge of alternative methods, it can **select or even invent** the best decomposition at runtime.

* **Contextual Knowledge from LLM:** We can leverage the LLM's knowledge for environment-specific decisions. Minecraft is full of contextual rules (e.g. it's dangerous to explore caves at night without a bed). The HTN planner can ask the LLM questions like "Is it safe to continue building at night?" or "What resource is needed to make glass windows?" and adjust the plan accordingly. In fact, GITM's *LLM Decomposer* and *LLM Planner* explicitly use **text-based knowledge** to inform subgoal breakdown. This ensures plans remain sensible: if night is falling, the plan might include *"skip building and make a temporary shelter or bed"* as a subtask for safety. *Ghost in the Minecraft* showed that by leveraging knowledge and memory, an LLM-based agent can handle varied scenarios (different biomes, day/night cycles, encountering monsters) **"with ease"**, precisely because its planning adapts to those conditions (e.g. equipping a sword or seeking shelter when needed).

* **Resource and Time Constraints:** The planning algorithm should consider not just *what* to do but *when* and *how long* it might take. Being resource-aware means if a task (like smelting sand into glass) will take a certain time (e.g. needing fuel and waiting for furnace), the plan might schedule that earlier or in parallel with other tasks. We can design the HTN such that long lead-time tasks (like smelting or growing crops) are kicked off early by the planner. An advanced HTN might even spawn background subtasks (e.g. start smelting iron while continuing to build structure) if the environment and agent capabilities allow concurrency. While classical HTN planners might not natively handle parallel tasks, an LLM could reason about it or the execution system can interweave tasks.

In summary, a dynamic, resource-aware HTN for Minecraft uses **conditional methods, LLM-informed decisions, and continuous state feedback**. At each decomposition step, the LLM or planner checks: "Given the current inventory, location, and time, what's the best next breakdown of this task?" This ensures that the plan is always grounded in the agent's reality and can change if the reality changes.

## Integrating Symbolic Planning with LLM Reasoning

Combining **symbolic planning** (the structured HTN approach) with **neural generation** (LLM reasoning) can yield the best of both worlds. The HTN provides a backbone of logical **constraints and hierarchy**, while the LLM contributes flexibility, creativity, and knowledge. Here are some best practices for this integration:

* **Define Clear Interfaces Between LLM and Planner:** One approach is to use the LLM at higher levels of abstraction and a symbolic planner or executor at lower levels. For example, you can have the LLM serve as a *high-level planner* that outputs a sequence of subgoals or actions in natural language, which are then interpreted by a symbolic layer that knows how to execute them. In GITM, this was done by splitting the agent into an **LLM Decomposer, LLM Planner, and LLM Interface**. The LLM Decomposer breaks the goal into subgoals (pure reasoning step), the LLM Planner orders specific actions for each subgoal (mix of reasoning and structured planning), and the LLM Interface is a non-learning module that turns those actions into actual game controls. This modular design keeps the LLM focused on what it's good at (reasoning, planning) and leaves the execution details to a deterministic system.

* **Use Symbolic Checks and Updates:** While the LLM might propose a plan, a symbolic component can verify its correctness against the current state. For instance, if the LLM suggests *"use 10 wooden planks to build walls"*, a symbolic inventory check can confirm the agent indeed has 10 planks. If not, the system can either prompt the LLM to reconsider ("You only have 5 planks, what now?") or automatically insert a *GatherWood* task. In this way, symbolic logic acts as a safety net for the LLM's suggestions, ensuring feasibility and consistency. Classic planning formalisms (like PDDL) represent preconditions/effects – we can use lightweight versions of these checks without writing a full planner by hand. Essentially, the **LLM generates candidate steps and a symbolic layer filters or corrects them**. This is similar to methods that augment classical planning with common-sense from LLMs (e.g. adding missing preconditions that the LLM identifies).

* **Prompt the LLM with Structured Plan Context:** To mix neural and symbolic planning, provide the LLM with a structured context in its prompt that includes the current **partial plan, goals, and state facts**. For example, the prompt could list active goals and known facts ("Inventory: {wood:5, stone:10}", "Time: night"), and then ask the LLM: *"Propose next sub-task for goal BuildHouse. Consider safety (night) and resources."* By explicitly giving these details, we guide the LLM's generation with symbolic state information. The LLM can output something like: *"It's night, so before continuing to build, craft a bed and sleep to skip until morning."* The symbolic system can then verify if the agent can craft a bed (does it have wool? if not, LLM might already suggest *"(if no bed) gather 3 wool from sheep"* as part of plan). This **tight coupling of factual state + LLM reasoning** yields plans that satisfy both logical requirements and creative problem-solving.

* **Limit the Action Space with Schema or APIs:** When mixing with neural methods, it helps to constrain the LLM's output format. Define a set of allowed action types or a "planning DSL" and include that in the prompt or fine-tune the model on it. This way, the LLM won't hallucinate completely unsupported actions; instead it will compose known primitives. For example, you might tell the LLM that the actions it can use include `[Move(x,y), Collect(item), Craft(item), Build(structure), Fight(entity), Wait(duration)]` and so on. The LLM can then combine these to form plans. This is akin to giving the LLM a symbolic **vocabulary of actions**, marrying the neural flexibility with a symbolic action space. *Ghost in the Minecraft* explicitly constructed a set of such textual actions for the LLM planner, which greatly improved reliability because the model plans with these discrete actions that the system understands (no free-form magic beyond them).

* **Iterative Refinement Loop:** A powerful pattern is to have the LLM generate an initial plan, then analyze or simulate it (possibly using symbolic rules or by stepping through mentally), and have the LLM correct any issues. This *plan-refinement* loop leverages the LLM's ability to do chain-of-thought reasoning. For example, the agent might ask the LLM: *"Here is my plan: 1) gather wood, 2) build walls... However, I have no axe to gather wood. How to address this?"* The LLM might then insert a step "craft an axe" at the beginning. This iterative dialogue between planning and checking can be seen as the LLM playing both planner and debugger roles. In literature, approaches like **DEPS** (Describe, Explain, Plan, Select) use an interactive loop with the LLM to refine multi-step plans and ensure they are achievable. Integrating such loops into the HTN planner means the agent can self-correct its plans before or during execution, mixing symbolic checks (like resource counts) with neural-generated fixes.

In practice, mixing symbolic and neural planning requires careful **prompt engineering** and possibly fine-tuning. You want the LLM to respect the structure (the hierarchy and constraints of tasks) but still be flexible enough to handle novelty. One successful strategy is providing **examples of hierarchical plans** in the prompt (few-shot learning), so the LLM sees how a complex task breaks down into subtasks. Another strategy is to use the **LLM for high-level open-ended decisions** (like "decide what the top-level subgoals should be") and then use a deterministic planner for ordering and low-level action sequencing. For instance, an LLM might produce a set of subgoals for "build a house" (like *StructureBuilt*, *InteriorFurnished*, *AreaSecured*), and then a traditional HTN method script or search algorithm can take those and expand into a detailed sequence using predefined methods.

In summary, the **best practice is to let the LLM handle the parts of planning that benefit from general knowledge and creativity, and let symbolic logic handle the parts that require precision and adherence to game rules**. By clearly defining how the two interact (through prompts, interfaces, and action schemas), we can harness the LLM's strengths while keeping plans grounded and executable.

## Plan Repair, Interruptions, and Replanning

No plan survives contact with a Creeper! In a dynamic sandbox, **plan repair and interrupt handling** are essential. The HTN agent must be prepared to pause or alter its plan when conditions change or if a subtask fails. Our design requirements specifically call for *interruptible and resumable plans*, as well as efficient real-time replanning. Here's how we can achieve that:

* **Execution Monitoring:** The agent should continuously monitor the execution of each action and relevant world signals. This could be implemented as a loop that checks after each action (or periodically) for certain triggers: health dropping low, an enemy nearby, nightfall, inventory becoming full, etc. If an important condition is met, an **interrupt signal** is raised. The HTN architecture can use a **stack** or **priority queue** of tasks to manage interrupts. For example, if a monster appears while building, the monitor might push a high-priority goal *"DefendSelf"* or *"SeekShelter"* above the current task.

* **Hierarchical Interrupt Handling:** Because we have an HTN, an interrupt can be treated as a new top-level goal that temporarily supersedes the original goal. The agent will then work on this new goal (using its own decomposition) until it's resolved, after which it can **resume the previous task** at the point of interruption. This requires the planner to save the state of the current task hierarchy (which subtask was in progress, what partial results are achieved). In implementation, one can suspend the current plan and later reinstate it. For instance, if *BuildHouse* was halfway done (walls built but roof not finished) when an enemy attacked, after dealing with the enemy the agent should go back and continue building the roof rather than restarting from scratch. The HTN representation makes this easier because the building task is an explicit subgoal that remains not yet completed; the agent just returns to it.

* **Plan Repair on Failure:** Not all interruptions are exogenous; sometimes the agent's plan itself fails. Maybe the agent expected to find coal in a cave but found none, or a tool broke, invalidating the next steps of the plan. In such cases, the agent should attempt to **repair or replan** for the failed subtask. A straightforward approach is **replanning from the current state**: treat the failed subtask (or the top-level goal if necessary) as a new problem and run the HTN planner again with updated state knowledge (e.g., "the cave is empty, try a different cave"). However, full replanning from scratch can be costly if done too often. A more efficient approach is to **reuse the portions of the plan that are still valid**, changing only what's necessary. Research in HTN planning suggests that reusing partial plans improves stability and efficiency. For example, if the plan was to *MineIron -> SmeltIron -> CraftBucket*, and "MineIron" failed because the vein was exhausted, the agent can keep the latter part (*SmeltIron -> CraftBucket*) and just replan the "MineIron" step (maybe choose a new location or method to get iron). This minimal adjustment avoids throwing away progress. Algorithms like SHOPFIXER and IPyHOP aim to **replace just the failed branch of the plan** rather than the whole plan. In our LLM-based system, we could implement a simplified version: detect which subtask failed and why, then prompt the LLM to suggest an alternative for that specific subtask. For instance: *"Failed to mine iron at location X (no iron left). Suggest a new sub-plan to obtain iron ore."* The LLM might respond with *"Substitute with: explore a new cave for iron"* or *"trade with a villager for iron"* if it has the knowledge, which the planner can then integrate in place of the old step.

* **Resuming Plans:** After an interruption or repair, the agent needs to merge back into the main plan seamlessly. If the plan was suspended, it can simply pop the suspended plan from the stack and continue. If the plan was altered (repaired), it will have a new sequence of steps from that point onward. In both cases, a robust design is to always recalcualte the next action based on the current HTN state and world state, rather than blindly following a stale script. That means continuously validating that the remaining plan is still applicable. If another change occurred in the meantime (say the time of day changed while fighting a monster), the plan to resume building should account for that (maybe it's now night, so place torches or sleep before building further). In essence, after any interrupt or failure, the **planner re-enters at the appropriate node in the hierarchy with updated world information**, and possibly re-decomposes that node if needed.

* **Emergency Overrides and Safeguards:** Some interrupts are critical (like low health). The agent should have **standing rules** to handle these regardless of the current goal. This can be implemented by high-priority methods that *always* have a chance to preempt if conditions are met. A practical technique is to check critical conditions at the start of each planning cycle or action execution. If any are true, immediately plan/execute the corresponding response (e.g., *HealSelf* or *Retreat*). These can be seen as *daemon tasks* attached to the HTN that fire when triggered. Ensuring these are integrated means the agent can avoid catastrophic failures (like dying) by intelligently interrupting its own plan.

* **Continuous Replanning vs. Plan Stability:** There's a trade-off between constantly replanning (responsive but potentially wasteful) and sticking to a plan (stable but potentially suboptimal when things change). A good strategy is to adopt a **continual planning** mindset: the agent is always planning a little ahead, executing, and re-planning as needed. You can limit the scope of replanning to what's necessary – e.g., only replan the current subtask or the remainder of the current goal, not the entire goal tree, unless the goal itself has become invalid. If the environment is highly unpredictable, shorter planning horizons with frequent updates can actually be more efficient. If it's relatively stable, the agent can plan longer sequences. The system might even learn how often it needs to replan by experience – for instance, if building at night always triggers an interruption, it might incorporate "if night, then pause building" into its default methods over time.

* **Real-Time Performance:** For real-time replanning, efficiency is key. We should use fast checks (symbolic conditions) and avoid extremely long LLM calls in the middle of urgent situations. One approach is to pre-cache some contingency plans. For example, have a plan for "if attacked" already formulated (perhaps via an HTN method or an LLM few-shot that's readily available) so that when it happens, the agent doesn't need to deliberate from scratch – it can immediately execute an evasive maneuver or fight sequence. Similarly, the agent can keep track of *alternative resources or paths* (plan B) in case plan A fails. This is akin to classical planners keeping a **plan library** or multiple methods for a task. An LLM-based agent could be prompted to generate a couple of alternative methods for each subgoal during planning, and store them. If the first method fails, it can quickly try the next one without a full new LLM query.

* **Concurrent Planning and Acting:** To maintain real-time responsiveness, the agent can **plan in parallel with execution**. While the agent is carrying out a low-level action (which in a game might take a few seconds, like walking to a location), the planner can use that time to compute subsequent steps or update the plan. Modern LLM agents often operate in a sense-think-act loop; we can tweak this to sense-think-*act\&think* concurrently. For instance, as the agent walks to a tree to chop wood, the planner could already be figuring out the next subtask (perhaps planning the crafting of planks). By overlapping these, we reduce idle time. Care must be taken to keep the LLM's context updated with any new perceptions during execution.

In sum, **plan repair and interruptions are handled by a combination of monitoring, partial replanning, and hierarchical control**. The HTN provides a natural way to resume tasks, since each subgoal's completion status is tracked. The LLM can assist by quickly suggesting fixes for failed steps or adapting the plan to new conditions. Empirically, maintaining some of the original plan structure tends to be faster than replanning from scratch every time something goes wrong, so our system should do the minimal necessary fixes to get back on track. This approach yields an agent that is **resilient** – it can recover from surprises like a Creeper explosion blowing up half-built walls, by immediately setting a new goal ("repair the damage") under the overarching project of building a house.

## Learning and Optimizing Task Decompositions from Experience

Designing the perfect hierarchy of tasks and methods upfront is difficult. An effective HTN agent should **learn from experience** – both from human demonstrations and its own trials – to improve its task decompositions over time. There are a few complementary ways to achieve this learning:

* **Learning from Human Demonstrations:** If we have examples of humans (or expert agents) accomplishing tasks (like building a house) step-by-step, we can use those to train the HTN. One method is to perform **behavior cloning** at the task level: infer the subgoal structure from observed action sequences. For instance, if many demonstrations of "build a house" show a pattern of first gathering wood, then crafting planks, then building walls, the agent can abstract that into an HTN method: *BuildHouse -> \[GatherWood, CraftPlanks, BuildWalls, ...]*. Researchers have explored algorithms to **learn HTN methods from unannotated demonstrations**, essentially extracting the hierarchical structure from flat sequences. This often involves identifying "landmarks" or key steps common across demonstrations and using them to segment the task. In practice, one could parse a human gameplay trace with an LLM, asking it to summarize the strategy in terms of subgoals ("The player first prepared materials, then built the frame, then added details"). The resulting candidate decomposition can be added to the agent's library of methods.

* **LLM Fine-Tuning or Prompt Tuning:** If you have a lot of demonstration data or prior plans, you can fine-tune the LLM on this distribution. For example, feed it many instances of user instruction -> decomposition pairs. The LLM will then internalize these patterns, making it more likely to produce optimal decompositions on its own. Even without full fine-tuning, a **prompt library** can be used: store a few high-quality examples of task breakdown (perhaps those the agent generated that led to success) and prepend them to future prompts. This way, the LLM is continually guided by what worked before – a form of **in-context learning from experience**. GITM's approach of recording successful action sequences into a text-based memory is an example of this strategy: the agent "remembers" what action plans succeeded in the past and can reuse or adapt them in the future.

* **Reinforcement Learning on Methods:** Once the HTN provides structure, you can use reinforcement learning (RL) at the level of choosing or refining methods. For example, the agent might have two different methods to achieve *BuildShelter* (one using wood, one using digging a hole as a quick shelter). Through experience, it can learn which method is more reliable or faster under what conditions (maybe digging is better on the first night, building wood house is better when resources are plenty). We can assign credit to the method choices based on success outcomes. A simple approach is a bandit or score for each method (success rate, reward achieved) and bias the planner to prefer those with higher scores over time. More complex is learning new methods entirely: perhaps the agent tries a novel sequence that works and then generalizes it into a reusable pattern.

* **LLM-Assisted Policy Learning:** A recent idea is to have the LLM propose a decomposition and then *learn a low-level policy for each subgoal via trial and error*. One example is the **DECKARD** system, where an LLM first breaks a task into subgoals, and then each subgoal is learned through gameplay (reinforcement learning or other techniques), and the initial decomposition is corrected if it was flawed. In other words, the agent doesn't assume the LLM's breakdown is perfect – it uses experience to refine it. If certain subgoals turn out unnecessary or if an order swap improves efficiency, the agent can update its HTN methods accordingly. For instance, maybe the LLM's plan for "build a house" always said to gather all materials first and then build, but through experience the agent learns it's better to lay a floor (which uses some wood) early (perhaps to have a safe area) before gathering everything else. The agent could modify the plan structure to intermix construction and gathering, based on reward feedback. Over time, the HTN thus **evolves to be more optimal** for the environment and the agent's capabilities.

* **Storing and Reusing Successful Plans:** In long-term play, the agent will accomplish many tasks. It should store the final plans that worked (this could be as simple as logging the sequence of actions that achieved the goal) in a **case library**. Later, when a similar goal arises, the agent can retrieve a past plan and adapt it instead of starting fresh. This is akin to **Case-Based Planning**, where past solutions guide new ones. For example, after successfully building one house, that plan (or sub-plans of it, like a blueprint) can be reused for the next house with minor tweaks (maybe different dimensions or materials if needed). This dramatically speeds up planning and increases reliability, because the agent is drawing from what it **knows works**. An LLM agent can be prompted with "Recall how you solved a similar task before" along with the memory content of that prior plan. By doing so, the LLM is primed to output a tried-and-true sequence rather than something random. Over time, the agent's memory becomes a rich source of skills – effectively the HTN library grows, and the LLM might eventually just act as a glue or fallback for novel situations, using learned decompositions for familiar tasks.

* **Human Feedback and Correction:** Since LLM-based agents can take natural language input, a human operator or expert can also correct the agent's plans, which is another way of learning. If the agent proposes a flawed plan, a human might say "No, don't build the roof last, put the roof before furnishing so you're safe from rain." The agent can incorporate this instruction as an update to its method ordering. Over iterative feedback, the HTN methods align more with human preferences (this is related to techniques like reinforcement learning from human feedback, but applied to planning steps).

In implementing learning, it's important to represent the HTN methods in a way that is easy to update – e.g., as data or scripts rather than hardcoded logic. One could use a data structure or even a small DSL (domain-specific language) for methods that the agent can modify or append to. Some researchers have explored algorithms to **autonomously refine HTNs** by adding or adjusting method preconditions and subtasks when execution experiences teach new constraints. For our purposes, even a simple approach like counting successes/failures for each method or storing demonstrations can yield significant improvements. The end goal is that the agent becomes **increasingly efficient** at task planning – early on it might make naive plans that work but are slow, whereas later it has learned shortcuts and better decompositions (for example, knowing exactly how much of each material is needed so it doesn't over-collect, or learning to handle routine interruptions like nightly shelter in a standard way).

By integrating these learning mechanisms, the HTN agent transitions from following manually designed task breakdowns to **discovering optimal hierarchies on its own**. This addresses the requirement of learning optimal decompositions: the more the agent plays (and/or the more it watches humans play), the more its internal library of task knowledge grows and the better its planning becomes.

## Example HTN Decomposition: *"Build a Furnished House"*

Let's walk through a concrete example of how an HTN planner (augmented by an LLM) might decompose the goal **"build a furnished house"**. This example highlights how the goal can be broken into structured sub-tasks and illustrates the considerations of resources and order. We will assume the agent is starting in a typical Minecraft world with some basic tools.

**Top-Level Goal:** Build a furnished house (a safe, enclosed shelter with basic furniture).

**Decomposition Outline:**

1. **Select Building Site** – *Find a suitable location* for the house.

   * Evaluate nearby area for flat ground, away from immediate dangers. If no clear area, clear some land (remove trees or level ground).
   * Mark the intended footprint of the house. *(Precondition: a safe, flat area is ready)*.

2. **Gather Construction Materials** – *Ensure all resources for building are collected*.

   * **Collect Wood**: Chop trees to gather logs. Convert some logs into wooden planks (for walls, floor, etc.). *Target:* e.g. 50 planks for a small house.
   * **Collect Stone**: If using a stone foundation or floor, mine stone or cobblestone. Also collect sand if glass windows are desired.
   * **Process Materials**: Craft any required blocks (planks from logs, smelt sand into glass using a furnace). Make sure to have a crafting table placed for crafting tasks. *(This step is resource-aware: if the agent already has some materials, it will top up only what's needed.)*

3. **Construct House Structure** – *Build the house's floors, walls, and roof.*

   * Lay the **foundation/floor**: e.g. place wooden planks or stone in a 6x6 outline.
   * Erect the **walls**: Build 4 walls at least 3 blocks high. Leave a doorway (2 blocks high, 1 wide) in one wall. Optionally, leave openings for windows (to be filled with glass).
   * Install **door and windows**: Craft a wooden door (if not already) and place it in the doorway. If glass was prepared, craft glass panes and insert into window gaps.
   * Construct the **roof**: Use planks (or other materials) to cover the top. Ensure the interior is fully enclosed to be safe from the elements and mobs. *(The order here is important – roof last allows easy movement while building walls, but the planner might decide to partially roof earlier if night is approaching.)*

4. **Craft Furniture and Amenities** – *Prepare interior furnishings.*

   * **Craft a Bed**: If the agent has 3 wool (from sheep) and planks, craft a bed for sleeping. If wool is missing, include a subtask to gather wool (shear sheep or kill for wool) as part of resource collection.
   * **Craft Storage**: Craft a chest for storage (needs 8 planks).
   * **Craft Workstations**: Ensure there's a Crafting Table (if not already placed) and craft a Furnace (needs 8 cobblestone) for smelting.
   * **Craft Lighting**: Craft torches (using coal/charcoal and sticks) to light the interior. If no coal, use wood in furnace to create charcoal.
   * (Optional) **Decorative/Utility Items**: Craft any extra items if resources allow – e.g. glass panes for windows (if not done), a door (if not done), maybe stairs for a nicer roof or floor.

5. **Furnish and Secure the House** – *Place the crafted items and finalize the house.*

   * Place the **Bed** inside (this also sets a respawn point in Minecraft mechanics).
   * Place the **Chest** in a convenient corner for storage.
   * Place the **Crafting Table** and **Furnace** inside, creating a small "workshop" area.
   * Place **Torches** or other light sources around the interior (and perhaps outside by the door) to ensure the house is well-lit (prevents monster spawns inside, and helps finding it at night).
   * Double-check that the house is fully enclosed (no open ceilings or holes) and the door can close. The house is now built and furnished.

6. **Finalize / Clean up** – *Any finishing touches.*

   * The agent could optionally decorate (place windows if holes were left, add additional aesthetic blocks, etc.), but these are extras beyond a "furnished, functional house".
   * Ensure any leftover materials are stored in the chest and the area outside is clear of hazards.

This decomposition can be visualized as an HTN tree where "BuildFurnishedHouse" expands into sub-tasks like "GatherMaterials", "ConstructStructure", "CraftFurniture", each of which expands further. During execution, the agent might intermix some steps (for instance, it might craft the door immediately after collecting wood, during the construction phase, rather than strictly separating phases). The HTN is flexible: if at step 3 night falls, the agent might temporarily jump to a subgoal "make temporary shelter or bed" before continuing with construction. Similarly, if a Creeper appears during step 5, the agent would interrupt to **fight or flee**, then resume furnishing.

**Why this decomposition is reasonable:** It mirrors how a human might tackle the task and takes into account resources and safety. All needed resources (wood, stone, wool, etc.) are gathered before or during the building process, so the agent won't get stuck. The plan explicitly includes crafting necessary tools and items (door, bed, etc.), so the end result is a functional house. It also sequences the tasks in a logical order – you wouldn't place furniture before the roof is done (or it might get rained on or a mob might jump in). The HTN formalism would treat some of these as ordered subtasks (you must build walls before roof), while others could be done in parallel or swapped if needed (crafting a chest could happen anytime before placing it).

For clarity, here's a more schematic HTN breakdown in bullet form:

* **BuildFurnishedHouse**

  1. *FindBuildSite* – ensure a flat, safe area. *(If area not safe: ClearArea or DefeatNearbyThreats)*.
  2. *AcquireMaterials* – gather and prepare all building materials.

     * Get wood logs (chop trees) -> craft planks.
     * Mine stone (for floor or furnace).
     * Get sand -> smelt glass (for windows) – *(optional if windows desired)*.
     * Collect wool (for bed) – *(if bed needed and wool missing)*.
  3. *BuildStructure* – construct the house's structure.

     * Lay foundation/floor.
     * Build walls (leave door opening, window gaps).
     * Craft and place door.
     * Build roof (cover house).
  4. *CraftInteriorItems* – craft furnishings and utilities.

     * Craft bed (wool + planks).
     * Craft chest (planks).
     * Ensure crafting table (planks) and furnace (stone) are available.
     * Craft torches (coal/charcoal + sticks) for lighting.
  5. *FurnishHouse* – place interior items.

     * Place bed, chest, crafting table, furnace inside.
     * Light up the interior (place torches).
     * Final touches (close any windows with glass, etc.).
  6. *VerifyCompletion* – check that the house is enclosed, spawn-proof, and all desired furniture placed.

This example would be fed to the LLM-based planner as either a guiding template or the LLM could generate something similar when asked about "build a furnished house". The hierarchical structure makes it easier to handle interruptions. For instance, if the agent's pickaxe breaks during "Mine stone", the subtask *AcquireMaterials* can include *"craft or obtain a new pickaxe"* as a repair step without abandoning the whole project.

## HTN Planning Algorithm (Pseudocode)

Below is pseudocode illustrating a possible HTN planning and execution loop for our Minecraft-like agent. This algorithm integrates the concepts of dynamic decomposition, interrupts, and replanning that we discussed. It assumes we have access to an LLM (for reasoning about subgoals) and some symbolic state checks (for conditions like inventory or health). The pseudocode is kept high-level for clarity:

```python
function HTN_Planner_Agent(goal):
    plan = []  # will hold the current high-level plan (sequence of actions)
    planStack = []  # stack for hierarchical tasks (for interruption and resumption)
    planStack.push(goal)

    while planStack is not empty:
        current_task = planStack.peek()  # look at the task on top of stack

        # 1. Check for interrupts or urgent tasks before refining current task
        if urgent := detect_urgent_condition():
            # e.g., low health, enemy nearby, etc.
            emergency_goal = handle_urgent_condition(urgent)  
            # handle_urgent_condition could return a goal like "DefendSelf" or "HealSelf"
            if emergency_goal:
                planStack.push(emergency_goal)
                # loop continues, emergency_goal will be planned next (higher priority)
            continue  # go back to loop to plan the new top-of-stack goal

        # 2. If current_task is an **action** (primitive) and executable:
        if is_primitive(current_task):
            if can_execute(current_task, state):  
                execute_action(current_task)  
                planStack.pop()  # action done, pop it
                # After execution, update state (inventory, location, time, etc.)
                update_state_after(current_task)
                continue  # move to next iteration to see what's next
            else:
                # Action is not feasible (precondition failed), trigger plan repair
                failure_reason = get_failure_reason(current_task, state)
                # Ask LLM or use alternative method to fix the failure
                alternative = suggest_fix(current_task, failure_reason)
                if alternative:
                    # Replace or insert new task to address the failure
                    planStack.push(alternative)
                    continue
                else:
                    # No fix available, abort this task
                    planStack.pop()
                    continue

        # 3. If current_task is a **compound task** (needs decomposition):
        planStack.pop()  # we'll decompose it, so remove it and replace with subtasks
        method = select_method_for(current_task, state)
        # 'select_method_for' uses either predefined HTN methods or LLM to choose a decomposition
        if method is None:
            # Ask LLM to decompose if no predefined method or method not applicable
            subtasks = LLM_decompose(current_task, state)
        else:
            subtasks = method.subtasks

        # 4. Push the subtasks onto the stack in reverse order (so first subtask is on top)
        for subtask in reverse(subtasks):
            planStack.push(subtask)
        # Loop continues, will handle the newly pushed subtasks in subsequent iterations
    end while

    # All tasks completed
    return "SUCCESS"
```

A few notes on this algorithm:

* It uses a **stack** to manage tasks, which naturally handles the hierarchy (depth-first expansion of tasks) and makes it easy to push interrupts on top. This is inspired by typical HTN and also Behavior Tree execution patterns.
* Step 1 checks for interrupts via `detect_urgent_condition()`. This could examine global state (health, time, etc.) each loop. If an urgent condition is found, we immediately push a corresponding emergency goal on the stack. For example, if `detect_urgent_condition()` sees health < 5, it might return a goal *HealSelf* or *FindSafety*. We push it so it becomes the new focus.
* Step 2 handles primitive actions. If the task at top is low-level (like an atomic action), we check if it's currently executable with `can_execute()`. That function might check simple preconditions (e.g., "do we have the item to place?"). If yes, we call `execute_action`, which interfaces with the game API to perform it (this could be through the LLM Interface or directly if we've scripted primitives). After executing, we update the agent's internal state.

  * If a primitive action **fails** (perhaps we thought we could do it but couldn't), we enter a failure handling section. We find out why (e.g., no path to target, resource missing, etc.) via `get_failure_reason`. Then we attempt to repair: `suggest_fix` could consult a set of alternative methods or even query the LLM: *"Action X failed because Y – what should I do?"*. The returned `alternative` could be a new task (like *"Find alternate route"* or *"Obtain missing resource"*). We push that on stack to address it. If no fix is found, we simply drop the action (pop it) – effectively abandoning that branch of the plan.
* Step 3 deals with compound tasks (high-level tasks that need to be broken down). We pop the current task (we're about to replace it with subtasks). We then choose a method: either a predefined one or by asking the LLM via `LLM_decompose`. Predefined methods might exist for common tasks (like BuildHouse as above), possibly with multiple choices depending on state. If none applies, the LLM is invoked to dynamically generate subtasks (it could return a list of steps in NL which we parse into subtask structures). We then push those subtasks onto the stack. We push in reverse order because stacks are LIFO – this ensures the first subtask ends up on top and will be planned/executed next.
* The loop continues until the stack is empty, meaning the top-level goal and all its sub-tasks are completed. The result is success (or we could detect specific failure modes if something was unachievable, but in a resilient system the agent might just keep adjusting until it succeeds or the goal is deemed impossible).

This pseudocode abstracts away some details (like how exactly the LLM output is parsed, or how the game actions are executed), but it captures the flow: **decompose, execute step by step, monitor for issues, adjust on the fly**. It's a mix of **depth-first HTN planning and reactive execution**. In practice, one might implement this with recursion instead of an explicit stack, but the stack form makes the pushing of interrupts easier to visualize.

For **real-time performance**, note that the heavy calls (LLM invocations) happen only when decomposing a new task or when repairing a failure. If the goal is large like building a house, the agent might do an initial decomposition (possibly via LLM) and then execute many primitive actions (e.g., 100 block placements) without needing the LLM until something unexpected happens. This is efficient because the expensive reasoning isn't happening at every step, only at decision points. If the world is stable, the agent just follows the plan. If something changes, the agent spends some compute (via LLM or method logic) to replan that part.

We could also introduce a separate thread or asynchronous call for `LLM_decompose` so the agent doesn't pause acting while thinking, but that adds complexity. The pseudocode is single-threaded for simplicity – in each loop it either plans or acts. In an optimized implementation, when the agent is executing a lengthy action (like walking a long distance), we could perform lookahead planning for the next steps concurrently. But conceptually, the above loop handles everything.

## Integration with Existing LLM Agents

Integrating HTN planning into an LLM-based agent architecture requires connecting several components: the LLM (for planning and reasoning), the environment interface (for sensing and acting in Minecraft), and the memory/learning modules. We can outline an integration strategy and draw from the design of systems like GITM and Voyager.

&#x20;*Comparison of a direct RL agent vs. a hierarchical LLM-driven agent. The LLM-based approach (right) first decomposes the goal into sub-goals and structured actions, then executes them via a low-level interface. This modular design leverages the LLM's reasoning at high level while using deterministic control at low level.*

**Architecture Overview:** A hybrid agent can be organized into layers:

* **Perception Module:** Responsible for observing the game state. In Minecraft, this might parse the game's JSON observations or video frames. The output is a symbolic or textual summary of the state (e.g. "Daytime, at coordinates (100,65,-30), health 8/10, saw a Creeper 20 blocks away, inventory: 5 wood, 2 stone…"). This state description is crucial for both the planner and the LLM's context.
* **High-Level Planner (HTN+LLM):** This is where our HTN planning algorithm runs. It takes as input the current **goal** (which could be set by the user or by some overarching objective) and the current state. The planner may query the LLM for decomposing tasks or for suggestions when needed, as described earlier. The result is a **plan**: a sequence of structured actions or an action tree ready to execute.
* **Low-Level Controller (Interface):** This executes the actions in the environment. For a Minecraft agent, this involves issuing commands via the game's API or simulating key presses/mouse movements for each action. The **LLM Interface** in GITM, for example, translates a high-level action like "mine 10 stone" into a series of key inputs (move to a stone block, click to mine, repeat). This component ensures the plans produced (which are in semantic terms) actually get realized in the game. It also handles action-level error checking – if a command cannot be completed (maybe the path is blocked), it reports back to the planner.
* **Memory and Knowledge Base:** A storage for the agent's learned knowledge – could include facts (crafting recipes, mob behaviors, etc.), past plans (successful strategies, pitfalls to avoid), and even world maps (locations of resources discovered). The LLM can retrieve from or be primed with this memory to improve planning. For instance, if the agent explored and found a village, memory might note "Village at (200, 70, -50) with a blacksmith (source of crafted items)". Next time the agent needs an item, the planner might use this info (maybe plan "go to village to trade for item X" instead of crafting it).
* **Goal Manager:** In a multi-goal or long-term setting, a component might decide what the agent's next top-level goal is (especially in an "open-ended" setting where agent sets its own goals). This could be a simple script or another LLM prompt (some projects use an LLM to suggest interesting goals, like Voyager which generates a curriculum of tasks). The goal manager feeds goals to the HTN planner one at a time, and could reprioritize if new important goals arise (similar to an interrupt but at a higher level, e.g. an externally given mission).

**Information Flow:** Integration means the outputs of one module feed into the next:

1. **State -> Planner:** The current state (from perception) is encoded and given to the planner/LLM. This can be done by constructing a prompt that appends state info (e.g., inventory, time, threats) along with the goal, and asking the LLM for a plan. Alternatively, the state can be used by symbolic logic inside the planner for method selection as we did in pseudocode. In either case, accurate state information is critical. In Minecraft, some state info might be large (the whole world map). We wouldn't dump everything into the prompt due to token limits; instead, we give relevant summary or a query mechanism. One might use retrieval-augmented generation: e.g., query a knowledge base for "what items do I have" or "where is nearest forest" and include that in context.
2. **Planner -> Controller:** Once a plan or next action is decided, the planner sends it to the controller. If using the LLM interface approach, the high-level action might itself be a piece of text that the controller can interpret. For example, the plan might say: *"ACTION: move to (105,65,-30) and mine coal"*. The interface module knows how to parse "move to (x,y,z)" and execute it (pathfinding or direct movement commands) and then "mine coal" (target nearest coal block and perform mining). In some implementations, the LLM could directly generate *code* for the controller to execute. For instance, Voyager's agent had the LLM generate Python code that calls into the MineDojo API for actions. HTN tasks could similarly be linked to code: e.g., each primitive task might have a function and the LLM's plan basically calls those functions in order. This is a robust integration: the LLM outputs a program (the plan) and a runtime executes it. If something goes wrong during execution (exception or unmet condition), control can be passed back to the LLM or planner to handle it.
3. **Controller -> State (Feedback):** As actions are executed, the environment changes. The controller and perception update the world state. Feedback loops to the planner are vital when something deviates from expected. For example, the planner expected an action to be done; the controller should inform if it's completed or if it failed (with reason). This feedback could be fed to the LLM planner by updating the prompt or by invoking a special "error handling" prompt. Integration here means deciding how the LLM gets informed: perhaps we maintain a conversation where the LLM is told "Action X failed because of Y" and asked "How do you want to proceed?". A tightly integrated design might also automatically trigger certain plans on failures without always involving language (for efficiency).
4. **Memory Integration:** The agent's memory should be updated with significant events – for instance, once the house is built, it might store a record of that success (including maybe how long it took, what materials were used, etc.). If the agent later needs to build another house, the planner can recall this memory to plan faster. Memory integration with LLM could be via embedding-based retrieval (vector store of past plans) or simply appending a summary of past successes in the prompt as context ("Recall: last time we built a house, we needed 50 planks and 20 stones, and it took 5 minutes."). This ensures the LLM has some temporal continuity and doesn't plan in a vacuum each time.

**Use of Existing Frameworks:** There are existing platforms like **Project Malmo** (by Microsoft) or **MineDojo** that provide a bridge to Minecraft for AI agents, exposing observations and actions via APIs. An integration strategy could leverage such a platform: the HTN+LLM planner runs externally, communicates with Minecraft through the Malmo/MineDojo API. The LLM itself could be hosted separately (via an API call to a model). The architecture might look like:

* Minecraft environment (with Malmo mod) <-> Python agent program (handles state, planning, action dispatch) <-> LLM service (like an OpenAI API or local model).

Within the Python agent program, you'd implement the HTN logic and tie it to the environment. For example, using Malmo's API, you can get a JSON observation of the world. You convert that to a state description, feed it to the LLM in a prompt along with the current goal, get back a plan or an action. Then you execute the action via Malmo commands. This cycle repeats. Essentially, the HTN planning algorithm we wrote in pseudocode becomes the heart of the agent loop.

**Alignment with LLM Agent Paradigms:** Recently, "LLM-based agents" often use prompting techniques like *ReAct* (Reason+Act) where the model reasons in text and outputs an action command. Our HTN integration can follow a similar pattern but impose a hierarchical structure on the reasoning. For instance, the prompt to the LLM might encourage it to think in terms of subgoals: "Explain how to achieve the goal step by step, then give the next action." This was done in the DEPS system, which had the LLM *describe, explain, plan, select* in a loop. We can incorporate that by having the LLM generate not just one action, but a **hierarchical plan** in its reasoning trace, and then commit to one action at a time. This way the LLM is effectively simulating an HTN planner in its chain-of-thought. Integration means capturing that chain-of-thought and possibly constraining it to follow the HTN format.

**Testing and Refinement:** When integrating into an actual agent, it's important to test in the environment and refine the prompts and code. Start with simpler tasks (like "gather wood") to ensure the loop works, then scale up to complex ones. Logging the dialogues between the planner and the LLM is useful for debugging – you may find the LLM misunderstood a task representation or used an action not supported by the interface. Those issues can be fixed by adjusting the prompt or adding more guidance (for example, explicitly telling the LLM "When planning, only use these verbs: gather, craft, build, etc.").

Finally, integration with existing agents could also mean adding HTN planning capabilities to an agent that might already have some learned policy. For instance, if there's a pre-trained model that can do low-level navigation or combat, the HTN planner can call that as a subroutine. The **hierarchical** design is inherently modular – you could plug in an RL-based combat agent to handle the *DefendSelf* subgoal while the LLM plans the bigger picture. This combination of modules (each possibly using different AI techniques) can greatly enhance performance.

In summary, the integration strategy is to have a **central planning module (with HTN+LLM)** that orchestrates the agent's behavior, with well-defined connections to perception (state input) and action execution (output). Systems like GITM have validated this approach: they used an LLM to decompose goals and plan actions, then executed them in Minecraft via a specialized interface, achieving much stronger results than pure RL. By following a similar modular design and ensuring the LLM's output is grounded in executable actions, we can imbue a Minecraft agent with both **the brains (reasoning)** and **the brawn (actual game interaction)** needed to tackle open-world tasks effectively.

## Conclusion

Bringing Hierarchical Task Networks into an LLM-driven Minecraft agent enables long-horizon autonomy that is both **structured and flexible**. We've outlined how to represent plans in an LLM-friendly way, decompose complex goals like building a house, and handle real-time challenges through interrupts and replanning. The key is synergy: using the **LLM's knowledge and reasoning to guide the HTN** and using the HTN's formality to keep the LLM grounded. Over time, with learning from experience, the agent becomes smarter in breaking down tasks optimally. This HTN approach addresses the major requirements – dynamic adaptation, interruptibility, resource awareness, and learning – all within the rich, unpredictable sandbox of a Minecraft-like world. With careful integration, such an agent can achieve impressive autonomy, as evidenced by recent research successes, and it serves as a stepping stone toward generally capable agents in open-ended environments.

**Sources:**

* Zhu et al., *Ghost in the Minecraft: Hierarchical Agents for Minecraft via LLMs*, ICLR 2024.
* GITM Project README – LLM Decomposer/Planner/Interface and results.
* Wang et al., *Describe, Explain, Plan, and Select (DEPS)*, NeurIPS 2023 – interactive LLM planning in Minecraft.
* Nottingham et al., *DECKARD: LLM-based Task Decomposition with Learned Policies*, 2023.
* Zaidins et al., *HTN Plan Repair (IPyHOPPER vs SHOPFIXER)*, HPlan Workshop 2023.
* Other cited works on LLM-assisted planning and HTN learning.