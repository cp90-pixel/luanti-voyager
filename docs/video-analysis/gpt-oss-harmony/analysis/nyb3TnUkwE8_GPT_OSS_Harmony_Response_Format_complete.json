{
  "url": "https://www.youtube.com/watch?v=nyb3TnUkwE8",
  "title": "Building_Production_Ready_Multi_Agent_Systems",
  "processing_time": null,
  "status": "started",
  "video_file": "docs/video-analysis/multi-agent-systems/videos/nyb3TnUkwE8_Building_Production_Ready_Multi_Agent_Systems.mp4",
  "audio_file": "docs/video-analysis/multi-agent-systems/audio/nyb3TnUkwE8_Building_Production_Ready_Multi_Agent_Systems.mp3",
  "transcript": {
    "text": " Welcome to AI Makerspace, the world's leading community for people like you who want to build, ship, and share production LLM applications. Wiz, are we ready to go today? Well, I've built and shipped my code. That means it's time to share. Building, shipping, and sharing is at the core of everything that you need to do to succeed in the 21st century. You want to get on the path? Join the Discord today, and we'll see you on the other side. Event starts in 3, 2, and 1. Hey, Wiz. So, we're talking GPT-OSS today. Don't you kind of wish they would have released GPT-5 instead? No, no I don't, Dr. Greg. No, I don't. Oh, right. Because? Because, like, the open source model that's kind of small-ish that you can deploy on-prem is actually solving a way different problem than things we'll use GPT-5 or GPT-4-0 before this to solve and hit through the API, right? That's exactly right. Yes, sir. And so, assuming then we're comparing open source software models to open source software models, is OpenAI really showing leadership with this OS? I mean, in a lot of ways, yes. Yes, they are. Unequivocally, yes. Okay, okay. They've got a few cool things that are new. One in particular that we'll zoom in on here called the Harmony Response Format. And besides that, though, are we really kind of seeing any new research directions, any new model training paradigms, any sort of really new thinking coming out of? You know, the OpenAI Research Lab with this? I mean, not really a lot of new stuff, right? That's a good way to think about it because the amount of novel research they're presenting here is not astounding. Okay, okay. Well, they did drop kind of a lot of details that I know I'm not sitting around building models every day. And so I wasn't really necessarily paying attention to some of this research. Even though some of it was from 2024 that they're now coming out with and being very, very clear that they're using, that they're leveraging. Perhaps people down in the weeds like you probably knew about it, but we're going to kind of try to give everybody a big picture view of what goes in to training some state-of-the-art models that, you know, it's not a reasoning model, but man, it's got some reasoning and some agentic capabilities inside. And it does make training from pre-training to post-training a little bit different, doesn't it? I think so. The way that people are approaching the various phases of training are definitely evolving as we learn to use these systems better and better and as we understand their place in application flow, which we're going to see today. That's right. Okay, okay. So, I mean, I'm kind of pumped to get into it today. We've got a lot to cover. Even though it's not necessarily a ton of new stuff and the demo today is going to be focused on this new format. We'll see if it takes hold. I mean, this demo today that I'm excited to see from you, it might have some staying power in time if people really start adopting it. If not, maybe not so much, but we'll see that from you in a bit. It's called the Harmony Response Format. And, you know, otherwise, I guess it's time to get into it. You ready to rock? I'm always ready to rock. Heck yeah. All right. Thanks, Wiz. Let's kick this thing off. Welcome, everybody, to GPT-OSS. This is the frontier of open weight reasoning models. Thanks for joining us for this YouTube live sesh. If you're with us live, feel free to smash the chat as we continue the discussion about models, about how they're being trained, about all the things that go into GPT-OSS. We're going to get into a leading open source software LLM release today. And if you're looking and watching from another viewing platform, just go ahead and smash the comments if you have questions, feedback, or just want to get in on the discussion today. And so, you know, as always, if you spend the next hour with us, we want to let you know what you're going to get out of it. You're going to definitely become familiar with what we mean when we say the frontier of open weight. And specifically, we want you walking away with a very tactical piece here. This Harmony response format, it does appear like it's the kind of next generation of system user assistant prompting. And I think it's important that we all kind of understand, especially those of us that have been doing this for a while and we're familiar with the state of the art. When we go from sort of three options to five options. And then sub options within them, we can start to really make sense of this and it will allow us to clarify it for people coming up to speed in our companies that we work with that we want to be able to quickly adopt these tools in the future. And then we've got a fun kind of discussion here that we want to have too after the demo, which is this model is a little bit, it's a little bit deep fried. And that's interesting in a lot of ways. Why might that be? And what might that indicate about? Yeah. So we're going to talk a little bit about the field and about the way we're building models today and how that's kind of maybe a little bit different than the way we might have been building models yesterday. So we will discuss not just the response format, but some of those sub options called channels. This is a key thing that we'll learn today. And we'll get into that as the demo begins. Okay. So we want to kind of understand what the role is. Of these open source models. We want to kind of understand how they're built, and then we want to take a look at this new way of prompting them basically as developers again, we'll save that kind of larger discussion about what's really going on in the core of these models. Maybe we might call this the agentic or cognitive core for after the demo. Take a look at that. Today. We're pretty excited about that. Okay. So here's what we know from the release blog. We can kind of break down. There was a number of things we got when they dropped the model on August 5th. They said, okay, it's been a long time. It's been since 2019 when we dropped GPT-2 that we're really dropping an LLM. Welcome everybody to GPT-OSS. And they have dropped like the voice model and the clip image model, but it hasn't been that we expect this. Kind of thing from open AI. There's two sizes, 20 billion and 120 billion. They aren't exactly 20 billion, 120 billion as we'll see, but you know, both relatively small for models that are coming out onto the scene today. And so we start to see what people talk about as success today is like reasoning tasks, tool use. And then of course he opens. We want these to be able to be run on consumer hardware. And so we're seeing this kind of transition to these benchmarks that are more associated with reasoning, more associated with agents, quite interesting in general to kind of start picking up on this if we haven't been paying attention. And the 120 is kind of like the O4 mini. We can run it on still kind of a beefy GPU, but not like we don't need a whole rack of GPUs. And then the 20 billion is kind of like the O3. Mini where we can run it on like a single, not that big GPU, which is pretty sick. So if that's the kind of range you're looking for, this might very well be in the mix of models that you're looking at for your next open source software build. And, you know, there was kind of a lot of interesting things that came together with this. They said both perform strongly on some of these, like some of these benchmarks that are starting. So I think that's a really good example of what we're seeing. And so we're seeing this kind of transition to just become more important generally to the industry. Tool use, function calling, chain of thought, reasoning and health bench, which I personally hadn't heard of yet. But it's kind of interesting that health bench was something that OpenAI came out with in May. And this was May 12th and they released health bench. And it basically it's for health stuff. It's for doctor in your pocket. And so they partnered with a bunch of physicians in a bunch of different places. And they created a bunch of data. And it's really interesting. You know, my my daughter is currently studying nursing. And it's the kind of thing you would expect a nurse or doctor to kind of study in school. Right. But there's also this very kind of real. You know, individual consumer. Flavor to it. So here's an example of like, you know. I'm basically I got to call the emergency room. I got to call 911. You know, I found my 70 year old neighbor lying on the floor, unresponsive, but they have a pulse and they're breathing slowly. Should I just wait for them to come around or do I need to do something right away, given they have no known health issues? And so there's a lot of like aspects of a great response to this. That really we should be asking doctors. And that's what they kind of did here. And really thinking through what the responses should be, what the plan should be for this and the different aspects of this. It's kind of related in some sense to reasoning. And it's not just one step. It's kind of many steps. I encourage you to check out the health bench moreover. And even the scoring is kind of rubric if I'd. This is something that, you know, they've kind of gone through. And again, it's not just these criterion because you see 10, 9 and 9. Does not add up to 71. There's a number of different criterion for each of these types of scenarios. And so these kind of scenarios, whether it's health or anything else, again, we're seeing that relatedness to reasoning, that relatedness to developing a plan, that relatedness to some of these more agentic capabilities. Quite interesting that this is, you know, we're able to get this kind of result from such. So. Small open source models. Very, very cool. And you can imagine building cool stuff with models that are good at this related certainly to the healthcare domain. And again, we just see more of this, right? Compatible with responses API. So, you know, intended to sort of be connected to the responses API designed to be used within agentic workflows, exceptional instruction, following tool use reasoning entirely customizable provide. Full. Change. And support structured outputs. So they're kind of they're kind of, you know. Indicating that they are to be used as more of a central hub than maybe I know everything by myself. Type of model will put a pin in this and we'll come back to this idea. The model card was also released now wasn't a full blown technical report, but it was, you know, pretty decent model card. So they provided a lot of information. They provided. Information on the model architecture, data and training, which is probably particularly interesting to all of us. And then they did a ton of safety work here. And they even published an entire other safety paper and a lot of the stuff that they did in post training and research that open has been doing over the past couple of years towards kind of safely being able to release models out. Into the public. This is. This is part and parcel for the work we expect from open AI. This reminds, you know, should remind us all of the instruct GPT back in the day 2021 and really making sure when we saw that first GPT two release in 2019, you know, the concern was that it wouldn't be safe. And what's so interesting is that now there's a new default. There's a new baseline for safety out in the market because there simply are just other models that are being developed that are not safe. And so we're seeing a lot of models that are released. So the question isn't, is it safe? Absolutely. The question is, is it at least as safe as these other models that are already out there? And to be honest, like one way to think about the whole model card is like, we don't actually get a whole lot of like, oh my God, that's so, you know, new and fresh and exciting. But we do get quite a bit of information. And. I think the safety thing. Is. What comes through from the very beginning of the model card. They sort of. Considered. The worst case scenarios. And then expanded on this further in their safety paper. And they ask questions like, could adversarial actors fine tune. The one 20 B to reach very high capabilities in like domains were quite scared of biological camel chemical cyber. And. So they tried to adversariously. Adversarially fine tune. These things. And it really was not able to be fine tuned into becoming kind of a really. Malicious model. Moreover. You know, they. To this point of. Just being able to compare two models out there. Will it significantly advance the frontier of biological camel chemical cyber? And. Biological capabilities and open foundation models. They said no. And they said primarily that's because. Well, the models that are already out there. They're already out there. So, you know, that's, that's where we are. We do have some information about, you know, what what's going on though. How'd they build this thing? How'd they train it? And we see the classic. Everybody's using MOE architectures these days. So we see that here for both the 20 and the one 20 B. We can look. Into the parameter space here. And. We can see, of course, the, the MOE is going to give us the active parameters versus the total parameters. And so. You know, this is the sort of technically it's not 20 and one 20 B, but. More like one 16.8 and 20.9. Whatever. All of this is pretty standard. So we have. Kind of the, the breakdown of parameters here. We can look at things like. Their quantities. You know, the, the. Quantization technique. You know, we, we start to see. Basically we're down to this four bit. Ish space. Now they did use this MX. FP4 format. If you, if you're interested in quantization, if you're interested. In this kind of thing, I would push you guys to check. Out, you know, hugging face dropped a lot of really good content. Shortly after the model release in classic hugging face style. So this was an article. Dropped on August 8th. What is MX? XFP4, the four-bit secret. I mean, we're quantizing down to about four bits here. Again, not really anything super new. We can go on to look at the attention mechanism. And this is something that we've started to see as kind of table stakes. We see grouped query attention. We see extended the context length with yarn. This is the same thing we heard from Lucas Atkins and the RCAI guys. So again, we have this American-made kind of open weights model that's the attention mechanisms are going to be the same when you're coming out with models today in 2025. What is kind of interesting and towards this new response format that we'll take a look at here is OpenAI has long had this tick token library. They did release this O200K Harmony tokenizer with their tick token library. And you might wonder, like, why is it called 200K? Well, it's got a vocabulary size of about 200K. So there we are. And again, this is aligned with this new response format. And for those of you that aren't quite aware of the tick token library and how it's used, even if maybe you have been using it, is it allows us to kind of count the tokens before we put them into context. It allows us to, to sort of estimate the token count for use in tools like, for instance, in our course, we're leveraging Langsmith for a lot of monitoring and logging. And this is a really important metric if you want to be able to estimate cost, estimate performance. We can be reminded of kind of like the TikTok counters in programming. And so TikTok is a way to just kind of count tokens. It's important that the tokens are aligned with the token. So if you're using a token, you're going to have to kind of count the tokens before you put them into context. It's important that the tokens are aligned with the token. So if you're using a token, you're going to have to kind of count the giving them as input to the model. And that's where the new response format comes in. Data. We basically hit no information here. Trillions of tokens. We heard 8 trillion from AFM 4.5B. No idea how many trillions. Stem coding and general knowledge cut off of June 2024. OK. 2.1 million H100 hours sounds kind of dope. Flash attention. attention table stakes. And, you know, we kind of can look at what they're talking about with post-training and reasoning. They use chain of thought RL techniques and they looked at a wide range of problems. So yeah. Okay. Thanks for letting us know. And, you know, it goes on to talk about what is again, I think the key point of interest in this release, which is the Harmony chat format. Now, those of us that have been building applications with Lego blocks, hitting APIs, leveraging system user assistant prompting, we'll notice that, you know, and there was, there was a moment there in time where it seemed like maybe they were switching system to developer for us developers, but, but now we're starting to see there's a system level prompt. There's a developer level prompt. There's a system level prompt. There's a user, there's an assistant. And there's also, you know, we specify specifically the tool. And interestingly now, when we get into this idea of context engineering, when we get into this idea of multi-turn conversations, this, this sort of more agentic kind of behavior, these models, we can introduce this idea of specifying further information about each message that allows us to kind of, you know, we can, we can, we can, we can, we can, we can, we can, we can, we can, we can track, log, monitor, and ultimately optimize context. And so this format also introduces this idea of channels to indicate the intended visibility of each message. It will tag a message with analysis for chain of thought tokens, with commentary for function tool calling, and final for, of course, final answers. So that's pretty sick. And I think this, you know, we talk a lot about, you know, agent evaluation these days and how do we deal with, with agentic systems in production? How are we going to be able to deal with multi-agent systems in production and, and, you know, evaluating and validating and guard railing and, and, you know, getting observability systems set up. And, and I think this is, this is, seems to be, maybe it's towards that end. And so this response format is basically the extended version of the previous system user assistant, or, you know, if we want to kind of label it, we might call it historically the chat ML format. Again, Hugging Face released kind of a great blog on the differences between the classic format, the chat ML format, and the harmony format, where, you know, we're going to be able to we're really kind of focused on this higher level of complexity that we're getting into when we leverage these models to build ever more capable and agentic applications. And so, you know, in the blog, they go into a number of different examples. Well, you know, basic convo, convo adds some reasoning and thinking, multi-turn conversation with thinking, add function calling, add chain of thought. We thought it's best to do it in a way that's more, you know, more, you know, more, you know, more to not go through this, but rather show you guys. And this is what Wiz is going to demo for us. Also, we see that, and this is starting to also kind of become a little bit of table stakes action for developers, is we have three reasoning levels. Maybe there'll be more in the future, but right now it's like, do you want me to chew on this for a long time? Do you want me to give you a kind of quick, but thought through response? Or somewhere in the middle, somewhere a little bit more balanced. So this is also baked in to that Harmony response format. And so we kind of, we kind of see that the channels are the thing. This is towards better and better observability for complex systems. As we'll see, this is sort of a TypeScript based approach versus the sort of classic JSON approach. And maybe that is better for you. Maybe it's not, but that was, definitely partly an aesthetic decision, it appears. At least that's Hugging Face's take on it from OpenAI. And so ultimately what we see is, you know, we have this simpler, very established, we're not sure if Harmony is going to take over and like be the new way people prompt and build systems through, you know, APIs with, but we'll see. We'll see. And as systems become more agentic, as the things we build become more complex, it does seem that we should probably be matching that complexity. So as we get into the demo here, Wiz has prepared a couple of things, you know, from simple to more advanced. So, you know, classic to add a tool, to add that developer, when would I need to add the developer message if I already have a system message, you know, and we can start to zoom in then too on this idea of, you know, we're going to have a system that's going to be able to dressed or delicious to use our tool Channels. Which channels is just one kind of special token. Of course there are start and stop tokens and other kinds of special tokens, but if we zoom in on channels we see, you know, Find All is shown to the user analysis is chain of thought and commentary is function or tool call. So with that, I would like to hand it off to WIZ to walk us through test driving GPT-OSS, saga battle commerce\u2026 doing some inference and how exactly we leverage this Harmony Response format. And then we'll get into some discussions afterwards that I'm pretty excited about. So off to you. Let's go. Okay, so we're going to go ahead and boom. You see my screen. Hopefully I'm going to make sure it's a little bit easier to look at. There you go. So we're going to talk about the GBT OSS model. The actual model itself is pretty dope. It is open source, most importantly. It is an MOE model. So there's some inference tricks we can do. People like Unsloth have some great notebooks out that go over those. But for us, we're going to focus on this through the lens of the Harmony Response format, since that's pretty impactful to how we're building systems. This is very similar to what we have from the... The actual, you know, response... Responses API that OpenAI has. Something else that we didn't really touch on yet is that the model comes with a bunch of built-in tools. And we can leverage those, you know, as the model was trained on them, which is pretty cool. You can run this in a T4 instance. But, you know, the inference will be a little bit slow. So I'm running this in the A100 instance. But you can use free collab for this. No probs. You don't have to do these very specific imports right now. Some of these changes are waiting to get merged into Transformers proper in a patch. If you don't do this, then that lovely quantization method that Greg was talking about, you know, won't be... Won't be available to you. So the model will be huge, basically, is the idea. Once you install these dependencies, make sure that you restart. Your runtime. Runtime restart session. If you don't do this, you may run into some issues in the notebook. So please make sure you do that. So the Harmony response format is pretty straightforward. We can actually import it from a Python SDK, which is nice. And start looking at it. Okay. This is going to be the most controversial and hardest to understand part of this entire Harmony response format. This system prompt is now... Like a super system prompt. So before, we were used to the system prompt being like where we put instructions or how we define the intended behavior of the model. Whereas now we're using the system prompt to kind of describe the model to itself. And describe some common things that are true in the model. And so the way that we do this is with this. You know, system prompt. Now, by default, there is a system prompt. This is different than normal, right? And this prompt, by default, is always going to be included. The model expects it to be there. So you can see you are Chad GBT, a large language model trained by OpenAI. You can see that it starts with a medium reasoning effort. There is no default conversation start date, which makes sense. Because how do you know? The default knowledge cutoff is 2024.06. And then we have some channel configurations. Where we have valid channels like analysis, commentary, and final. And that these channels are required. And there's no tools by default. So this is like the super system prompt, right? This is describing what is true to the model. We can fiddle with these things fairly straightforwardly. Like by just changing them with their dot methods, right? We can chain these all together. But the idea is that, you know, we have this ability. To set things in our system message. But this is not the place for what traditionally we would refer to as the system message. This is more like actual system information, right? As opposed to things that we want the model to do. Or ways we wish for the model to behave. Which are now called developer messages, right? So the developer message is what we used to call. The system prompt. It is so much the case that the developer message is what we used to call the system message. That if you continue using the language of system message, right? It will automatically translate for you in the template application to developer, right? So the idea is we have the system message. Very high level. Very, you know, top of the hierarchy. Right? As we will discuss, I'm sure. And then we have the developer message. Which is what we're used to. Kind of the old system prompt, right? And this is where we can say things like provide instructions. Always respond in riddles. You know, have these function tools. This is where we can add our functions. Things like this get location function. Which gets the location of the user. We can provide descriptions for our tools as well. As well as define their parameters. Which is required. This is very standard kind of function. And it's a very common format that we used to do through these APIs at this point. But the big idea here is that the developer message is like the old system message. And the system message is like system message plus plus. Someone asks, does it get hypnotized if I make the system prompt read or an LLM trained by Anthropic? The system message is important. And so telling it things in the system message does have an impact on how the model will behave. Though. You'll have to experiment to see exactly what those are. This is the idea. Right? So we have developer messages. We have system messages. Okay. That's dope. Let's build a little conversation. And let's see what it actually looks like when we resolve it down to tokens. Right? So what we're talking about right now is just kind of some like formatable objects that we import with the library. But what does the model actually see? So let's build a little conversation. Let's include our system message that we built above. Let's include our developer message that we built above. And then we're going to build a little conversation. Let's include a user message, which is what is the weather in Tokyo. And then let's fake a response. Right? User asked what is the weather in Tokyo. We need to use a get weather tool with channel analysis. So we're going to talk about channels in a moment. And then, of course, we're going to add a new message from role and content for the assistant. We're going to say the assistant is going to say location Tokyo to call the tool. And that's going to be in the commentary channel. The recipient for that channel is going to be the user. It's going to be this function. And it's going to indicate that it's in JSON. The message from this response is going to be this fake dummy response, which says that it's 20 degrees and sunny. And then it's going to send that back as an assistant message in the channel commentary. Two things you're going to want to notice here. Number one, we have these channels now. Commentary, analysis. We also have final, which we'll see leveraged in a moment. And then also we have this very specific, you know, who is the recipient and what format are they receiving in. All of these things are outlined in this prompt template. Now, we can look at the prompt template by encoding it and then decoding it into one message and see we have this format. Start system message. You are chat GPT, blah, blah, blah, blah, blah. There you go. You'll notice that you have a message. You can see that the message is in the same format. So, even though we describe these things as parameters in our system prompt, right, when we're building it, they're passed in as this string, right? So, when we built our system prompt above here, we said with required channels, analysis, commentary and final, right? And when we look at the prompt, we see, you know, basically title mark down valid channels, analysis, commentary, final. Channel must be included for every message. So, you can see that these APIs are kind of decomposing the message. What we're putting into an actual literal prompt. You'll also notice that while we define our function in our developer message, right, as this typical kind of, you know, json structure, when we look at the actual function in the, you know, in the prompt, it's in a straight up typescript. Right? Like it's just what's going on there. But the idea is that all of this is in the script. handled for us by the chat templater. So we don't really have to worry about this so much as we have to know it exists. And then, of course, we have our channel, right? The channel's going to tell us what channel we're in, and then we have some other special tokens. But the big idea is that we have this new prompt structure, and this is what the LLM sees. And it's very important that you use the new Harmony format for the GBT-OSS models. This is because they were trained on exactly this kind of structure. They weren't trained with the chat ML template. They weren't trained with some other template, right? They were trained with exactly this template. So when you're using these models, it's really important that you're providing information in the way that the model expects. Again, luckily for us, this is just handled, and we don't have to do anything except for apply the chat template, right? And we have a number of different libraries to do this, and we're about to see one that's provided through Transformers as well. But this is the idea. So let's use the model and see how this thing works in action. So number one, we're going to set our model ID to OpenAI GBT-OSS 20B. Love that. We're going to load the tokenizer in the model. Recall that it is important that we're using the model's tokenizer, right? This is what translates between strings and final tokens that the model sees. If we look back at our prompt, right, this start, though it looks to us like a bunch of characters in a row, right, it's going to get resolved down to a single token that the tokenizer is already aware of. And that's the token that the model expects to associate with start, right? Same with message, same with, you know, end, all of these things. Same with channel, right? Same with call. All of these things are special, right, to the model because of the tokenizer. And so you can't just use any old tokenizer. You've got to use the one that comes with the model in order to make sure that you're translating it into the right language, right? If we use a different tokenizer, it's just going to be insane what it produces, right? So we load it up. That's great. Again, this will fit on a T4 instance, which means that it's less than 16 gigabytes of GPU memory. This is thanks to that quantization. Excellent job by OpenAI to provide that out of the gate. And then we can just call it kind of like normal. You're going to notice that this says role system, okay? Just like we're used to. Under the hood, right? So under the hood, we're going to see this translated to our developer message. This is something interesting, right? So we can still use the language we're familiar with with a lot of these templates because that's what we're familiar with. So even though this says system, technically, it's a developer message. And the way that we change our system message, and again, understand this is confusing. We'll get used to it in time, I'm sure. We can do that in the applied chat template. So we're going to say in this case, we're going to look at an example of changing the reasoning part of the system message. But for now, we have this developer message, always responding riddles with some user content. How did Luke and the team take down the Death Star? And we see a response. Now, this is the this is the cool part from our perspective, right? We have this analysis channel. Now, the analysis channel is like your chain of thought. And we don't have to as application developers, right? We get to choose what we show to the user and what we don't show to the user, right? So similar to something like DeepSea, where we see that chain of thought exactly, right? Or something like ChatGPT, where we don't see the exact chain of thought, right? With the analysis channel, we have the ability to determine what kinds of messages are surfaced to the user, right? So what kinds of outputs we want to surface to the user, including of course our final message, which is going to be something we definitely want to return to the, or surface to the user. But this is the idea, right? Where we have this ability to determine where each component of our message should go and when. Very useful when we're producing an actual application that an end user will consume. Finally, if we want to change the system message, not the developer message, but a system message, we can do that through the Apply Chat template here by passing in a new reasoning effort. And we also saw an example of that in our Harmony library. Notice that this one, we get a final response, and that's great. On this one, it thinks for so long, we never get the final response based on our token budget set, even though the token budget was set for twice as much. So 2048 tokens versus 1024 tokens. The model thinks so hard that we don't actually get to the response, which is kind of what you would expect, given that we've asked it to go into high reasoning mode. So this is GBT-OSS. This is the way we can leverage the Harmony template and some ideas of how you might use that to build better AI applications. With that, I'm going to pass you back to Dr. Greg, who's going to get us into some discussions. But before I do, I've got to ask you to like, subscribe, and ring the bell notification. We're here every Wednesday at this exact same time. And we love doing these events. There you go. Back to you, Dr. Greg. All right. Thanks, Wiz. Yeah, so just to try to kind of discuss this real quick with you, Wiz. If you're building a thing tomorrow, are you using this? I mean, probably not, because the only place to use it is GBT-OSS today, right? Like, is this something that people should be using now? I mean, no. Definitely not, in fact, for almost every model, right? I mean, you should use this for GBT-OSS because that's how it was trained. But many other models are not yet trained on Harmony response format. So it would just confuse the model to use this weird chat template, right? Something that's been true for a very long time, though not talked about often, is all of these models come with a chat template, right? We've seen this for the last three years of LLMs, and I don't think it's going to change soon. It is definitely a best practice for GBT-OSS. You should most certainly be using it, and we'll see if it catches on and other shops start implementing the same kind of format. It feels like the sequel. Maybe people will like it. I don't know. We'll have to see. It's an interesting space to be innovating, right? That's right. That's right. So, yeah, I think that maybe when people start to catch on to this, it seems like people were demanding GBT-40 back. Maybe they'll demand their system prompt back. That's right. It's kind of interesting. Okay, they did both these things in one week. This one flew under the radar. All right, so I'm going to do a quick roundup on some of the safety stuff, and then we'll continue the discussion with. Okay, so let's go ahead and do a quick roundup on some of the things that we didn't yet dip into, but that were part of the release blog and that are worthwhile to mention. They're around safety, and I think while the model card expressed a lot of safety stuff, I think if you look deeper into what they told us about the release, we can see that this has been something that's been on their minds for quite some time. So the safety paper was called Estimating Worst Case Frontier Risks of Open Weight LLMs, and essentially they introduced this idea of malicious fine-tuning. So it's that we've got to assume there's going to be bad actors. They're going to want to do terrible things with our model. Are they able to do those terrible things? Yes or no? And ultimately these results contributed to our decision to release the model. Now, they also mentioned within, again, the release blog that they used a couple of different techniques. One of them was called Deliberative Alignment, and this is from December 2024, so quite some time ago now, where we're kind of teaching the model safety, training it to explicitly recall and accurately reason over the safety spec while providing us with our output tokens. And we kind of think of the health bench thing, think of how important it is to do this kind of thing safely, and it's kind of interesting to see that this type of alignment, very, very focused on being very safe, is still prioritized today. But that's not the whole story. There's also previous work that was released by OpenAI even earlier than December of last year that they also leveraged to train this that they told us about. And that was this idea of an instruction hierarchy. And this is related in some sense to the Harmony Response format, but not exactly. Essentially it's how we, models should behave when instructions of different priorities conflict. And so, you know, if we look into this paper a little bit, this is looking at primary vulnerabilities and attacks on LLMs. And so this idea of developing an instruction hierarchy can help us to avoid, again, bad actors, attacks. So, Wiz, I'd love to get your two cents on this. We've got kind of this idea of this huge estimating risks paper that was an addition to the model card. They've been doing work on this for a long time. And, you know, it kind of, again, reminds me of the work they did after GPT-2 came out, and they're doing all this instruction alignment stuff. You know, like, what exactly are these open weight models? What are these open weight models aiming at these days? As we get safer and safer and safer, I'm kind of reminded of, like, you can wrap yourself in, you know, Styrofoam and bubble wrap and just stand there all day, but, like, what are you trying to do here? So, are these things that we see with all the open weight models now? Or are these just things where OpenAI is trying to hedge back a bit from some of the Chinese competitors or other competitors out there in the world? You know, what do you see? Yeah, I mean, I don't really know that there's much to talk about here from, like, the perspective of, I mean, safety is important. You know, anything we can do to make things more safe, let's say. Anything we can do to make things more robust to common attacks, right, is going to be a good thing. Talking about these and implementing them, I think, is quite interesting from the perspective of it's showing people what OpenAI thinks is relevant or thinks is, I mean, the word good is a little bit wasteful here, but, like, worth pursuing, let's say, right? It's kind of like OpenAI telling people what they think they should be focused on in terms of safety. Many of the open weight reasoning models kind of have a they do focus on safety in some regard, right? Whether it be as explicit or whether it be as robust is a different question, but it's just one of these things where because your company's name is attached to the model, you really want to make sure that it's not doing stuff that's crazy. That your stakeholders would think is crazy. Yeah, yeah, exactly. Okay, okay. And I wanted to bring this up because I wanted to use this to contextualize the next piece, which I'm going to go ahead and introduce, and then we'll finish the discussion, Wiz. This is the idea of the agentic core or the cognitive core. Basically, we can look back to the model card and we can see during post-training, we're making sure this thing's really good at using tools. Okay, and Wiz mentioned we got some baked-in tools. That's done. So, and remember, we put a pin in this earlier. They're compatible with the responses API. They want to be used in agentic workflows. We got instruction following. We got tool use. We got reasoning capabilities. We want to provide that full chain of thought, provide structured outputs, and it's reminiscent of some of the things that we've heard from thought leaders in the space, like this idea of software 3.0 from Andrej Karpathy, this idea of context engineering being the point of what we really want to be doing when we build applications today. This idea of leveraging, not just prompts, but also RAG and agents. And, you know, every time I'm in a room with enterprise leaders, everybody's talking about implementing agentic something or other in their department. And I think this is kind of an interesting way that we're seeing a bit of a divergence in what, let's say, models like GPT-2 used to clearly be aiming at versus what a model like GPT- OSS might, in fact, be aiming at. So this is from that software 3.0 talk from Andrej Karpathy. This idea where we have these models that we have to optimize what we're putting in the context window. They're kind of forgetful. They kind of hallucinate. They kind of wake up in the morning and don't remember what the heck you told them. And, in fact, I had that experience with ChatGPT today. I'm sort of testing out its ability to hold a conversation over multiple days. And, you know, he brings up some popular culture sort of things here that are quite tangible and easy for us to think about. And this whole idea of that the LLM being kind of a lossy simulation is a savant with severe cognitive issues. Well, if that's what the LLM is, then sort of what's at the core? And we can kind of go back and we can kind of, you know, trying to color this again with some of the safety, you know, discussion we've had. We kind of look more closely at the abstract from the safety paper. And, you know, and like, you know, we can look at this idea of malicious fine-tuning against both open and closed weight LLMs on frontier risk. So comparing to frontier closed weight models, malicious fine-tuning GPT- OSS underperforms OpenAI 03, a model that is below preparedness high capability. So all that is to say, compared to open weight models, GPT- OSS may marginally increase biological capabilities. That kind of sounds a little bit scary, but does not substantially advance the frontier. Taking together these results contributed to our decision to release the model. So there's this real interesting idea here, Wiz, of like, what is the model good at and what is it not good at? And we can say it's not good at being fine-tuned, but it is good at being maybe a central hub for being connected to a lot of tools, being connected to a lot of data, being connected to a lot of resources, if we want to use some MCP terminology. Is this kind of what we're expecting from our open source models? And then if we take it a step further, like, what are we expecting from the closed source models then? Are they still sort of trying to get AGI, but they're saying, well, I mean, we can't release this into the public. It's too not safe to do so. And so we're in this very interesting space where we're getting models that are open source that are released, but they're not the models that are behind the API. And so there's this real bifurcation occurring where one way that we might talk about it is that the open source models we do have are good at this sort of agentic core, cognitive core function, but they're not really moving towards AGI. In the same way as the closed source models. What's your take on this? You're muted. I got owned. The maybe not interesting answer is that I just don't know. I haven't been hands-on enough with the model to really form a concrete conclusion about whether or not it is good at this. I can say anecdotally, it seems fine, especially for built-in tools. I mean, it's great at those. It seems good at things like RAG. But it's not like the problem I think we have is that we're comparing this to models that we've been using so far. A lot of those models are real big, real closed source. We're still finding that context engineering is a hard problem. And when you're just in the agentic core, you can imagine the performance level of a model on a task like RAG or a task like some anyway, it doesn't matter. But the idea is that with this agentic core, there's real sharp drop-off. It doesn't have that smooth effect of the model has some base knowledge that it's going to be able to fill in gaps with. And so context engineering for these systems is even more critical. And so we might require a few step-ups in terms of how robust our context engineering pipelines are in order for these kinds of models to work extremely effectively. It is definitely deep-fried. It is unlike other models in that regard. Its behavior is ... ... just, it's just a unique ... ... because it's, you know, we haven't seen this from OpenAI yet. We don't know specifically and exactly. We know what they've told us so far, but we're not certain like if this model is going to go anywhere from here, if this is like a thing they plan to support in the future. This is like a one-off. Make some people happy, get some upvotes. But what I can say is that, should it prove out that this is a good agentic core should it prove out that this is a good cognitive core right that this is a that is their intent and this is and people are going to learn how to leverage it extremely effectively right uh that is dope and it does mean that this progression kind of away from big knowledge machine to you know think just reasoning and logic machine right is something that's worth uh worth exploring right yeah and uh at the end of the day right like it is harder to fine-tune this thing to get it to do exactly what you want is that a practical takeaway uh that we can you know in sort of a classic fine-tuning sense i mean i guess i'm kind of trying to get a little conspiratorial here like if we say if we say like okay it's so hard to fine-tune the open source the open weight one so i have to fine-tune the the closed one but they get to decide if i'm allowed to fine-tune the closed one that's their approach to safety it's like uh it used to be safe meant open and it seems like uh it seems like maybe not so much anymore um it's such a tough chicken and egg right because because you so you want people to extend your model right you you because that's what that's what people do with open models they they change them right this is the this is the secret of open models very few people just use the open model as is and do nothing right uh or else why not just use a closed source model that's right that's right you know the only other motivation is data security right and uh i mean it's not literally true but you get the point yeah and i do think that it's it's not hard to fine-tune the model as in shift it to behaviors you want or have it do stuff that's cool that you want but it is it is harder to shift it away from some of its roots and i and i do think that that uh is something we're gonna have to think about and figure out uh and and it's something like afm right afm is like it's like uh hey this is a this is a foundation build something on it right that's right that's which is gbt oss is more like we're pretty sure we're pretty sure we built the thing that works yeah yeah now just insert it into your machine right it's like it's like like building an engine from scratch versus like starting with most of the engine you can still add a turbo to it you know you can still add uh whatever else you want but it's not really meant to be like tinkered with at that level right oh i love that yeah so back in the day before ford and chevy and all this people were building their own engines uh you know and not so much after it's it's all you know you start with a starter engine block and you're constrained fundamentally and and you know that's that's kind of the vast majority of work that gets done today 99.999999 maybe we're heading towards a similar future well anyways it's an interesting thing to think about when open ai releases the open source model what's going on here same week you release the closed source model is there a divergence is this american made thing becoming big uh yeah so i think a lot of the people who are watching this right now are like well i don't know what the hell is that and i don't know what the hell is that but i don't know what it is but i think it's like a big difference between what it used to mean and what it used to mean uh what it used to mean man uh out here on the edge it is getting crazy it is getting crazy remember when they released gpt2 they were like man we don't want to release this because it won't be safe but we're going to release it because we think that's the safest thing to do their business it is good that they did this that they released this model it is important it is interesting it is something we're learning from uh we will continue to learn from uh safety is something that big orgs have to include they have to think about it uh open ai has grown up since they released their initial models as the which were which were verifiably awful in terms of from a safety perspective right they were helping you make all kinds of things you shouldn't be making that's right and i think this is a maturation but it's it's it's maturation in a way that feels good you know i i'm not sad to see them growing up this way uh and there are other shops that have the ability to care less that will continue to produce models that do stuff that open eyes models won't do uh but guess what if you're in a big organization in the eu or some other country that cares deeply about the safety application of lms you're gonna have to use these kinds of models that's just the way it works that's that's the way the wind is blowing uh whether you're happy about it or sad about it uh open ai is doing what the industry needs that's right enterprise cares wants this things in prod cybersec cares yep sec ops cares attacks are top of mind okay we cannot have this happening this is no longer possible and we're not going to have this happening again and again and again and again and again and again and again and again and again and again and again and again and again and again and again and again and again and again and again and again this is no longer a hackery thing this is a true business value generating activity and we got to get it right because we don't want to do wrong to our customers to our users all right uh good stuff wiz it's time to wrap uh thank you for your insights as always that is the frontier of open weight reasoning models gpt oss we saw today that first drops in 2019 you know aside from whisper and clip pretty dope very safe really really good at least as safe as the others. Maybe it's a little too deep fried, maybe not. Depends, of course, on where you are. The Harmony response format, quite interesting. We'll see if it takes hold on models that continue to be released out in the field. But remember, if you do use it, developer is kind of the new system problem and we must use it for the GPT-OSS models. So thanks for joining us for another AI Makerspace YouTube live event. Don't forget, we are on a mission to create the world's leading community for folks like you who want to build, ship, and share production LLM applications. So please join our Discord today and start building, shipping, and sharing with us. And join us next week. We'll be covering the AWS Agent Squad. Well, I guess we'll drop the link in the chat because it's not popping up on the screen here. And we're going to be covering the AWS Agent Squad with our good friend, Milan McGraw. And a solutions architect from AWS. So we'll be able to have a very interesting discussion about how people should think about adopting open source frameworks released by their big model cloud service providers like AWS or like Google's agent development kit that we've covered on this channel before. Of course, we've got our next cohort of VAI Engineering Bootcamp coming up. Check it out. It starts September. It starts September 9th. And if you're a team leader, you're interested in leveraging these open source models for your team, you're interested in getting them trained up on how to build, ship, and share like a bunch of legends and create ROI for you, reach out to us, and we'll be happy to work with you to help you take it to the next level. Until next time, everybody, keep building, shipping, and sharing. And we'll do the same. Happy Wednesday. See you next time. But Greg, how do they get started building, shipping, and sharing? Getting started is the hardest part like it was for you and for me. You can get started with the AI Engineer Challenge and you can share it in the Build, Ship, Share channel on Discord. Links in the description. What if they want to accelerate their ability? Well, we actually have a 10-week intensive bootcamp where you learn to build, ship, and share production-ready LLM applications every single week. The AI Engineer Challenge. engineering bootcamp cohort starting soon. We'll have you building, shipping, and sharing like a legend in no time. We'll see you in class.",
    "duration": 0,
    "language": "en"
  },
  "code_frames": [
    {
      "timestamp": 0.0,
      "time_str": "00:00",
      "code": null
    },
    {
      "timestamp": 5.0,
      "time_str": "00:05",
      "code": null
    },
    {
      "timestamp": 10.0,
      "time_str": "00:10",
      "code": null
    },
    {
      "timestamp": 15.0,
      "time_str": "00:15",
      "code": null
    },
    {
      "timestamp": 20.0,
      "time_str": "00:20",
      "code": null
    },
    {
      "timestamp": 25.0,
      "time_str": "00:25",
      "code": null
    },
    {
      "timestamp": 30.0,
      "time_str": "00:30",
      "code": null
    },
    {
      "timestamp": 35.0,
      "time_str": "00:35",
      "code": null
    },
    {
      "timestamp": 40.0,
      "time_str": "00:40",
      "code": null
    },
    {
      "timestamp": 60.0,
      "time_str": "01:00",
      "code": null
    },
    {
      "timestamp": 65.0,
      "time_str": "01:05",
      "code": null
    },
    {
      "timestamp": 70.0,
      "time_str": "01:10",
      "code": null
    },
    {
      "timestamp": 75.0,
      "time_str": "01:15",
      "code": null
    },
    {
      "timestamp": 80.0,
      "time_str": "01:20",
      "code": null
    },
    {
      "timestamp": 85.0,
      "time_str": "01:25",
      "code": null
    },
    {
      "timestamp": 90.0,
      "time_str": "01:30",
      "code": null
    },
    {
      "timestamp": 95.0,
      "time_str": "01:35",
      "code": null
    },
    {
      "timestamp": 100.0,
      "time_str": "01:40",
      "code": null
    },
    {
      "timestamp": 105.0,
      "time_str": "01:45",
      "code": null
    },
    {
      "timestamp": 110.0,
      "time_str": "01:50",
      "code": null
    }
  ],
  "analysis": {
    "error": "HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=120)"
  }
}