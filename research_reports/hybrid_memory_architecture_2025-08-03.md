# Memory System Architecture for Lifelong Learning Agents in Open-World Games

**Source**: OpenAI Deep Research  
**Date**: August 3, 2025  
**Prompt Reference**: [DEEP_RESEARCH_PROMPTS.md - Prompt #5](../DEEP_RESEARCH_PROMPTS.md#prompt-5-hybrid-memory-architecture)

## Overview and Design Goals

Open-world lifelong learning agents require a **comprehensive memory system** to accumulate knowledge and skills over millions of in-game experiences. Inspired by human cognition, we design a memory architecture with distinct components for **working, episodic, semantic, procedural, and social memory**. This system enables the agent to **remember past events, generalize knowledge, learn new skills, and interact socially** – all while operating in real-time under limited compute and storage. Key design goals include: scalable storage (up to millions of memories within ~1GB), <100ms query retrieval, support for multi-agent (distributed) scenarios, controlled forgetting/consolidation, and the ability to perform analogical reasoning. The overall approach merges the strengths of large language models (LLMs) with structured memory modules, leveraging LLMs' knowledge and reasoning as *semantic memory* and external databases for experiential memory.

## Architecture Overview

**Figure 1** illustrates the proposed memory architecture. The agent's **Working Memory** serves as a central hub for **short-term context** – it holds the agent's current observations, goals, and any retrieved knowledge needed for the task at hand. Percepts from the environment flow into working memory (much like a CPU's registers or RAM), and decisions/actions are output to the game controller from this workspace. Surrounding the working memory are four long-term memory modules: **Episodic Memory** (specific experiences), **Semantic Memory** (general world knowledge), **Procedural Memory** (skills or behaviors), and **Social Memory** (knowledge of other agents). These long-term memories interface with working memory via fast retrieval and update operations (typically through vector searches, database queries, or learned indices) so that relevant information can be loaded into working memory in <100ms. Each memory module has a specialized role but they are tightly integrated – for example, an agent can retrieve an episodic event and then use semantic memory to infer its general lesson, or load a learned skill from procedural memory to execute it. During "offline" periods (analogous to sleep), a **consolidation process** transfers knowledge between memory systems (e.g. distilling frequent episodic patterns into semantic facts, or converting mastered experiences into new procedural skills), and performs pruning of less useful memories to stay within storage limits.

*Figure 1: Proposed multi-memory architecture for a lifelong learning agent. Working memory (short-term) is the central buffer for the current context. Long-term memory modules (episodic, semantic, procedural, social) store different types of knowledge and are queried or updated as needed. Dashed arrows indicate **offline consolidation** processes (compressing episodic experiences into semantic knowledge or new skills). Solid arrows indicate **online** memory access during acting/thinking (perception into working memory, retrieving past experiences, querying facts, invoking skills, recalling social info, etc.).*

## Memory Types and Representations

* **Working Memory (WM)** – *Short-term task context.* Working memory holds the **immediate situation** the agent is facing – recent observations, the current goal or sub-goal, and any information retrieved from long-term memory that's relevant to the current decision. In an LLM-based agent, WM corresponds to the prompt/context window (and any scratchpad variables) that persist across turns. We allocate a limited-capacity structure (similar to a blackboard or buffer) that is continuously updated as the agent perceives new inputs and takes actions. For efficiency, WM stores **high-level features or symbols** (not raw pixel data) – e.g. the current room description, nearby entities, current step in a plan, etc. Working memory may be implemented as a set of key-value pairs or tensors representing the state, which are directly fed into decision-making logic (such as the LLM prompt or a policy network). It is **volatile** – entries last only for the duration of the current context or until they are moved to episodic memory. This design aligns with human short-term memory (limited capacity, quickly updated). The WM module is responsible for packaging queries to other memories (e.g. forming an embedding query for episodic memory) and integrating their results. Notably, working memory **persists across decision loops** to accumulate context beyond a single action, enabling coherent multi-step behavior. Data structure: a small in-memory store (could be a Python dict or C++ struct in implementation) with fast access, since it's on the order of tens of items. Algorithmically, updating WM is O(1)/O(n) for adding or replacing elements, and the contents are directly used in <100ms loops.

* **Episodic Memory (EM)** – *Long-term store of specific experiences.* Episodic memory records the agent's *history* – a log of events, observations, and outcomes the agent has encountered (e.g. "Day 5: saw a dragon by the river and fled"). Each entry is typically time-stamped and rich in context. Because storing raw experiences (images, full state) can be expensive, each episode is **encoded** into a compact representation. Effective encodings might include: a **vector embedding** of the state or observation (using a neural encoder), combined with some metadata (timestamp, location, involved entities), or a short **text summary** of the event generated by an LLM. For example, a game event "agent crafted a wooden pickaxe after trying stone" could be summarized to "crafted wooden pickaxe (failed stone first)". These encodings are stored in a vector database (for similarity search) or a graph database (linking events by time/relations). We favor a *dense vector store* for efficient retrieval: e.g. FAISS or HNSW indexes allow similarity search among millions of vectors in milliseconds. The agent retrieves past experiences by embedding the current situation or query and finding **similar past events** (or by filtering metadata like location or involved NPC). This gives the agent **episodic recall** – the ability to remember "what happened before in a similar context." Empirically, adding episodic memory helps agents avoid repeating mistakes and improves planning. To manage storage (<1GB for millions of events), we compress each memory aggressively (each entry might be ~500 bytes). We also implement **forgetting**: unimportant or redundant episodes are discarded or merged over time. For instance, if the agent has 1000 nearly identical "wood chopping" experiences, it can keep a representative sample and drop the rest, or maintain only a summary statistic. The episodic memory supports **surprise/novelty tagging** – each experience can be tagged with a surprise score (e.g. high prediction error or unexpected outcome). These tags help retrieval algorithms prioritize memorable, informative events. Data structure: an approximate nearest-neighbor index over fixed-length embeddings (e.g. 256-D float vectors) for fast similarity queries, plus an attached database storing event details. Algorithms: insert (on new experience) which computes an embedding and adds to index (amortized O(log n)); retrieval which given a query vector returns top-K similar memories (O(log n) for ANN search). Additional processes handle periodic memory maintenance: e.g. reservoir sampling or clustering for older memories to prune them.

* **Semantic Memory (SM)** – *General world knowledge and concepts.* Semantic memory holds abstracted facts, schemas, and relationships that the agent has learned. This is the agent's **encyclopedic knowledge** about the game world: properties of objects, rules of the environment, common strategies, and any learned generalizations from episodes. We implement semantic memory in two complementary ways: (1) **Implicitly**, via the pretrained knowledge in a large language model, and (2) **Explicitly**, via a structured knowledge base that the agent builds over time. The LLM itself (e.g. GPT-style model) can serve as a semantic memory in that it contains vast prior knowledge and can answer factual queries. However, the LLM may not know game-specific details or newly learned facts, so we maintain an explicit store for those. The explicit store could be a **knowledge graph** of triples (subject–relation–object) or a semantic network of concepts, allowing symbolic queries ("what can cut a tree?") to retrieve facts ("an axe can cut a tree"). Alternatively, a simpler approach is to maintain a text-based knowledge base (documents or embeddings) of important facts discovered (like an in-game wiki). For example, after multiple episodes encountering a dragon, the agent might store a fact: "Dragons are vulnerable to ice magic." Such facts can be generated by **memory consolidation** (see below) or by prompting the LLM to summarize what was learned (as done in *Reflexion* agents). We ensure semantic memory remains much smaller than episodic (storing only *general* information, not single-use events). It might contain on the order of thousands of entries (concept nodes or text snippets). Retrieval from semantic memory can use symbolic matching (for graph queries) or embedding similarity (for text). The agent queries semantic memory for *background knowledge* to aid decisions – e.g., before attempting a task, it fetches known recipes or tool properties. This retrieval can be integrated via prompt augmentation (for an LLM controller) or as features for a policy network. Data structure: a graph database (like Neo4j or a Python networkx graph) or a lightweight vector store for textual facts. Algorithm: retrieval is typically O(1) for direct symbol lookup, or O(log n) for vector search among facts. Semantic memory is also where **analogical reasoning** is enabled: because it stores abstract relationships, the agent can map a new situation to a known schema (e.g. "building a bridge is analogous to building a roof" if the structural relationships are stored). By representing knowledge in a structured form, the agent can perform reasoning (with the LLM or logic) to draw analogies and infer solutions to novel problems based on past knowledge.

* **Procedural Memory (PM)** – *Skills, behaviors, and how-to knowledge.* Procedural memory encodes **how the agent does things** – essentially the learned policies or action sequences for tasks. In a game context, these are recipes, plans, or control policies the agent has mastered. For example, a procedural memory entry might be a skill like "build a shelter", which encapsulates a sequence of actions (gather wood, then build walls, then roof) possibly parameterized by context. Implementation-wise, procedural memory can take multiple forms: (1) as **chunks of executable code or scripts** that the agent learns and reuses (e.g. a macro in a scripting language or a Python function the agent wrote). This is exemplified by the *Voyager* Minecraft agent, which stored new Python functions for skills and retrieved them later via code search. (2) As **policy networks** or controllers trained via reinforcement learning for specific skills. For instance, after many experiences, the agent might train a small neural network to navigate mazes, which becomes a stored skill it can invoke. (3) As **plans/templates** stored symbolically (e.g. a saved plan outline for "obtaining a diamond" that was successful before). We combine these approaches: simple tasks may be stored as parameterized scripts or plan templates, while more continuous control skills are stored as learned subpolicy models. Procedural memory is accessed when the agent needs to *decide how* to achieve a goal: the agent's planner/LLM can query "do I know a procedure for this?" – if a suitable skill is found (by name or by embedding match of the goal description), the agent can retrieve it and either execute it directly or use it to guide the LLM's action generation. For example, if the current goal is "build a fence" and the agent has a similar skill "build a wall", it might retrieve that and adapt it (analogous to how humans recall a known procedure for a new but similar task). Procedural memory updates happen through **learning**: when practice or new experiences refine a skill, the agent either updates the skill representation (overwriting or annotating the script/policy) or adds a new variant. We budget space by storing only *high-level skills* – subroutine-like behaviors that can be reused – rather than entire raw action histories (those live in episodic memory). Data structure: a dictionary or library of skills indexed by keywords and embeddings (for semantic search of relevant skills). Possibly a repository on disk of code or a list of neural network weights for policies. Retrieval algorithm: similar to episodic, use dense embedding retrieval to find the skill whose description best matches the current objective (complexity manageable as number of skills will be far less than raw episodes). Execution of a retrieved skill may involve calling a function or running a network (constant-time once loaded). Fine-tuning the agent's LLM or policy network itself can be seen as updating procedural memory at the deepest level (the model's weights encode procedural knowledge too), but full online re-training is expensive, so our design leans on adding modular skills instead, for efficiency.

* **Social Memory (SMem)** – *Knowledge of other agents and interactions.* Social memory is specialized semantic/episodic memory focused on **other agents** (NPCs or player avatars) in the environment. It stores information like: who the other agents are, what interactions have occurred, what their dispositions or typical behaviors are, alliances or rivalries, etc. This is crucial in open-world games with multiple agents or characters – for example, remembering that "Merchant Bob trades rare items at high prices" or "Ally Alice helped in battle last week." We implement social memory as a **profile** for each known agent, which includes *semantic attributes* (learned or given facts: e.g. role, personality traits, trust level) and a link to relevant *episodic records* involving that agent (past interactions, dialogue transcripts, cooperative successes or betrayals). Essentially, social memory can be thought of as a filtered view of episodic and semantic memory keyed by agent identity. Each time our agent interacts with someone, the event is stored in episodic memory *and* indexed under that agent's profile. The profile may also contain summaries – e.g. "Bob: a gruff blacksmith, prefers honest deals. Last seen in village." We use lightweight structures like dictionaries mapping agent IDs to their info, and for retrieval, simply pull the profile or search within it. Social memory retrieval needs to be very fast (often just a lookup by agent name or ID, O(1)), since social context can directly affect dialogue or cooperation decisions. We also include a **social graph** linking agents (who knows whom, factions, etc.) as part of semantic memory – this helps the agent reason about relationships (e.g. "if I help Alice, Bob (her friend) will like me more"). Social memory is updated whenever new information about an agent is obtained (meeting them, observing an action, receiving a message). Over time, this memory may prune outdated info (e.g. if an agent changed behavior) and keep current beliefs updated (possibly through decay of old trust values if not reinforced). From a data standpoint, social memory typically occupies little space (proportional to number of agents, which is usually limited). Integration: before social interactions, the agent will recall from social memory the relevant facts (like the other agent's name and any important history) so it can behave appropriately (akin to a conversational agent recalling user info to personalize dialogue). This capability was demonstrated in *Generative Agents* where believable social interactions emerged from agents recalling past dialogues and relationships.

## Memory Encoding and Storage Efficiency

**Efficient encoding** of information in each memory type is critical to meet the scalability and speed requirements. We choose representations that balance information content with compactness:

* *Working memory:* stores high-level, task-relevant variables (small strings or IDs, neural activations of current state) instead of raw sensor data. This keeps WM on the order of kilobytes.
* *Episodic memory:* uses **embedding vectors** (e.g. 256-1024 dimensions) for similarity search plus a brief synopsis. Each episode might be encoded to a few hundred bytes. For visual experiences, the agent might store a visual feature vector (from a vision model) rather than the full image. For compound episodes, we can compress sequences by storing key states/transitions rather than every frame.
* *Semantic memory:* facts are stored as short text or triples (e.g. "<Dragon> vulnerable_to <Ice>") – highly compact. We avoid duplicating knowledge already in the LLM; semantic memory mainly holds *newly learned or domain-specific* facts.
* *Procedural memory:* a learned skill might be a small function (~a few KB of code) or a neural net (we'd aim for lightweight models, possibly tens of KBs if using tiny networks for specific tasks). Moreover, we only keep the latest/best version of a skill – if an old method is superseded by a better one, the old can be discarded to save space.
* *Social memory:* each agent's profile is essentially a small record (perhaps a few KB of text data summarizing key points). Only significant interactions are kept.

Given an upper bound of 1GB, the system might budget ~500MB for episodic memory (allowing on the order of one million episodes at ~500 bytes each), ~200MB for semantic memory (tens of millions of facts, far more than likely needed), ~200MB for procedural (hundreds of skills or some fine-tuned model weights), and the rest for social and overhead. We also implement **compression** techniques: for instance, periodically **summarizing clusters of similar episodes** into one prototypical episode or into semantic facts. This way, as episodes accumulate, older ones that are redundant can be archived or represented by a summary, freeing space for new unique experiences. The agent could also use a **generative model** to compress memory (train a smaller model on experiences so it can regenerate or recall them if needed, similar to model-based compression).

**Forgetting** is treated as a feature, not a bug – by design, less useful memories fade out. We use metrics like usage frequency and surprise/importance to decide what to keep. As noted in cognitive studies, *"forgetting is the natural and inevitable cost of storage"*, whereas *"memory consolidation… strengthens memory traces"* for important items. Our agent mimics this: frequently retrieved or high-value memories are strengthened (retained with higher detail), while those rarely used get pruned or only kept in abstract form.

## Memory Retrieval Strategies for Recency, Relevance, and Surprise

To make effective decisions in real time, the agent's memory retrieval must return the **right information at the right time**. We employ a retrieval mechanism that balances **recency**, **relevance**, and **surprise/importance** of memories:

* **Recency:** The agent tends to recall recent events by default, under the assumption that the latest experiences are likely relevant to the current context (especially in a non-stationary world). We implement a bias for recency by, for example, segmenting the episodic memory index so that the most recent X entries are always searched or by boosting the scores of memories from the last hour/day. In practice, a rule-based filter might always include at least one memory from the past N minutes if available. This ensures the agent doesn't "forget" what just happened in extended engagements.
* **Relevance:** We use embedding similarity between the current situation (or query) and candidate memories to gauge relevance. This is the core of vector retrieval – it finds experiences or facts that *match the context*. For instance, if the agent is currently in a cave looking for gold, the episodic memory system will retrieve other times the agent was in a cave or dealt with resources, due to high vector similarity. Similarly, semantic memory might retrieve the concept "gold ore spawns in caves at layer 20." Relevance is vital for targeting the search to a manageable subset of memory.
* **Surprise/Importance:** Some events that are not recent might still be crucial because they were surprising or highly important (e.g. a deadly trap encountered once, a unique clue found long ago). We assign an importance score to memories (this could be computed by an LLM reflection or based on reward/penalty received in that episode). At query time, we allow high-importance memories to be considered even if they are older or less similar on surface. The retrieval algorithm can compute a combined score: for example, `score = α*similarity + β*recency_norm + γ*importance` and rank accordingly. The Generative Agents work by Park et al. (2023) followed a similar approach, combining recency (time-based), importance (LLM-evaluated significance), and embedding relevance into a single retrieval score. This ensures *rare but critical* experiences aren't drowned out by a flood of recent trivial events.

Concretely, when the agent queries episodic memory, it will: (1) filter or bucket memories by recency (e.g. consider the last 100 and a sample of older ones), (2) calculate embedding similarities to find the top matches, and (3) adjust those scores by each memory's importance weight. A memory that was marked "high importance" (maybe it involved a mission-critical failure or success) will get a boost. This way, the agent might recall a year-old event of finding a secret key when facing a similar locked door, even if thousands of other events have happened since – because that event's importance flag and embedding match both cue it as relevant. We also incorporate **contextual cues**: the agent's working memory can act as a cue for episodic retrieval (similar to how humans use context to trigger recall). For example, working memory might hold "Quest: Cure the king's illness" and this cue leads episodic memory to retrieve "earlier found a rare herb in Dark Forest" which is relevant to curing illnesses.

Memory retrieval is implemented with algorithms like *sparse search combined with dense vector search*. We might first do a fast tag-based filter (e.g. match the location or task tag via inverted index) then the vector similarity on the filtered set, applying recency/importance adjustments as a final step. All of this is optimized to return results well under 100ms by limiting the search scope and using efficient ANN libraries.

Once retrieved, memories are loaded into working memory (possibly in summarized form to fit the working memory capacity). The agent's reasoning module (which could be an LLM) is made aware that these are retrieved memories (to avoid confusion between recalled info vs current percepts). For example, we might reserve a section of the prompt or a buffer in working memory labeled "Recall" where these items are placed, ensuring the agent knows to use them as reference.

## Memory Consolidation and "Sleep" Processing

To maintain long-term performance, the agent periodically performs **memory consolidation**, analogous to how humans consolidate memories during sleep. In our design, consolidation is a background/offline process that serves several purposes: compressing memories, extracting general knowledge, and transferring knowledge into more efficient representations.

During consolidation (which we might schedule during idle times or a notional "sleep" cycle each day of game time), the agent's memory system will:

* **Review recent episodic memories** and distill them into *summaries* or *lessons*. For example, if across the day the agent attempted building 5 types of bridges and 3 collapsed, it might consolidate this into a semantic memory entry: "Bridges need strong support in the center to not collapse." This uses the LLM in a reflection mode: the LLM is prompted with the day's episodes and asked to generate high-level insights. Those insights are stored as new semantic knowledge. This mirrors how the hippocampus (episodic store) teaches the cortex (semantic store) in biological brains.
* **Strengthen important memories**: For episodes deemed very important (high reward or critical error events), consolidation can create a *permanent record*. This might mean storing them in a separate "core memory" list that is never deleted, or increasing their importance score so they are very likely to be retrieved later. In the brain, repeated replay of important events during sleep is thought to strengthen those memory traces; our agent can explicitly re-encode such events (e.g. re-save their embedding with slight variations, or fine-tune a small model on them to imprint them in weights).
* **Merge similar memories**: The system identifies clusters of episodes that represent the same underlying situation. It then merges them into one prototype or summary. For instance, if an agent has 50 experiences of fighting spiders successfully, these can be merged into a single prototypical "spider fight" memory (maybe noting the range of outcomes) plus a count of how many times it happened. This drastically reduces storage while preserving the essential knowledge (the agent doesn't need all 50 identical logs). Techniques like hierarchical clustering or reservoir sampling can be used. We ensure that outliers (surprising experiences) are not averaged out – those might be kept separate because they could contain unique lessons.
* **Transfer to procedural memory**: Consolidation time is also used for *learning new skills*. If the agent has repeated a certain sequence of actions many times with success, it's a candidate to become an explicit skill. For example, after chopping down 100 trees across different episodes, the agent "knows" how to chop trees well – we can create a **TreeChop()** skill in procedural memory (either by training a specialized policy or by making a generalized script). This frees the agent from having to reason step-by-step next time; it can just invoke the skill. In a sense, this is turning slow, deliberative knowledge into fast, compiled knowledge – analogous to how practice leads to proceduralization in humans. Some works call this "chunking" in cognitive architectures (Soar, ACT-R) where frequently used action sequences become one production rule.
* **Model fine-tuning**: If using an LLM or other models, the agent could use off-duty time to fine-tune its models on accumulated experiences. For instance, a smaller transformer could be fine-tuned on dialogue interactions to improve its dialogue policy (which then effectively stores those interactions in weights, a form of procedural memory). However, given limited compute, we would do this sparingly. Possibly the agent collects "experience replay" and fine-tunes overnight on critical cases, similar to how some RL agents do offline learning. One example is the XTX approach where a model is fine-tuned on top trajectories to improve performance. Fine-tuning is expensive, so in our system this is optional and carefully scheduled (maybe only during long rest periods or when performance on certain tasks is degrading).

Consolidation thus serves to **prevent catastrophic forgetting** by integrating old knowledge rather than just dropping it. It also **clears the clutter** – by abstracting many experiences into a few summaries or skills, it keeps the memory store efficient. In an open-world game running for weeks, without consolidation the agent would be overwhelmed by raw data. With consolidation, the agent's knowledge becomes richer while the volume of stored data grows sub-linearly.

We draw inspiration from human sleep: the agent might have a concept of "end of day" where it triggers these processes. If real-time operation is required continuously, consolidation can happen incrementally (e.g. every hour process a batch of recent memories for a few milliseconds each tick, to avoid big pauses).

## Analogical Reasoning and Knowledge Transfer

Our memory system explicitly enables **analogical reasoning** by supporting retrieval of structurally similar knowledge from the past. Because semantic memory contains generalized schemas and relationships, the agent can map a new problem to an old schema. For example, say the agent has never encountered a lava moat around a castle before, but it has encountered rivers around villages. The agent's semantic memory might have a schema like "water around area -> need bridge to cross." By analogy, it can infer "lava around area -> perhaps build a bridge or find another way over." This inference is possible if the agent has stored the concept of *moat* and *bridge* in semantic memory. Similarly, episodic memory might recall a specific instance of crossing a river by building a raft. The agent can abstract that experience ("used a floating device to cross liquid obstacle") and apply it to lava (maybe "use a heat-resistant platform to cross"). Our architecture supports this by: (1) tagging memories with abstract relationships (obstacle->solution pairs, cause->effect, etc.), and (2) using the LLM to perform analogy mapping. We can prompt the LLM with something like: "Given the current situation X, find a past situation that is structurally similar and describe how its solution might apply." Because the memory retrieval will surface candidates, the LLM can evaluate which one shares a pattern. For instance, it might retrieve "In puzzle 5, aligned mirrors to direct light" and see that as analogous to the current "align pipes to direct water" puzzle.

The combination of **episodic + semantic** memory is crucial here: episodic provides concrete cases, semantic provides the generalized schema. This mirrors cognitive science theories (like case-based reasoning), where new problems are solved by adapting solutions from the most similar past cases. Our system can do case-based planning: use episodic memory to fetch a past plan that worked for a similar goal, then adapt it. Because procedural memory stores general skills, sometimes an analogy reduces to selecting an existing skill for a new purpose (e.g. use the "build bridge" skill learned for rivers to cross a chasm in a cave).

In summary, analogical reasoning is facilitated by **rich memory representations** and an LLM capable of making comparisons. By retrieving multiple related memories (some may only be partially relevant), the agent's reasoning module can find common principles. For example, retrieving "built wooden bridge across river" and "threw grappling hook to cross between rooftops" might let the agent see the common principle of creating a link between two sides, applicable to the current scenario. This addresses one of the hardest aspects of open-ended games: coming up with novel solutions by analogy rather than pure trial-and-error.

## Integration with Transformer-based LLMs

We integrate transformer-based large language models into the memory system to leverage their powerful reasoning and natural language capabilities. The LLM essentially functions as the **cognitive kernel** that can process and combine memories, but we avoid using it as the sole memory (due to context length limits and lack of long-term persistence). Instead, we adopt a *hybrid approach*: the LLM is used for *semantic memory retrieval and reasoning* and for *decision-making/planning*, while episodic/procedural memories are stored outside the LLM and brought in as needed.

In practice, this means: when the agent needs to decide an action, we construct a prompt that includes the current observation (from working memory) along with **retrieved memory snippets**. For example, a prompt to the LLM could be: *"Current goal: fix the broken gate. Observation: you have wood and nails. Recall: last week you fixed a fence with similar materials."* The LLM, seeing that recalled information, can infer the correct plan (use wood and nails to fix gate) by analogy to fixing the fence. This is essentially **Retrieval-Augmented Generation**, treating episodic and semantic memory as an external knowledge base for the LLM.

We also position the LLM itself as a part of the memory system: in M2PA's design, for example, the LLM was used *as the semantic memory module*. We similarly allow the LLM to answer factual queries directly using its internal knowledge (e.g. if asked "what's the capital of France?" the LLM knows that, no need to store in semantic DB). The agent's controller can have a simple policy: if a query can be answered by the LLM's own knowledge, use it; if it requires specific game experience, query episodic memory. This gives efficiency and breadth.

Moreover, the LLM plays a key role in **memory encoding and consolidation**: we use it to generate summaries and reflections. For instance, after a complex episode, we prompt the LLM (in an analyst role) to summarize what happened and what was learned, which we then store. This natural language summarization is useful because it compresses details into human-understandable form. It also helps produce the "semantic labels" (like importance ratings or category tags) for experiences. *Generative Agents* leveraged GPT-3.5 to dynamically summarize memories to keep the context window small – our agent will do similarly, using the LLM to maintain a running **"memory of memories"** that can be inserted into prompts (e.g. a synopsis of the agent's recent experiences).

**Transformer-based models for perception and procedural tasks:** In addition to a language model, we can use smaller transformer models for perception encoding (like a Vision Transformer to embed images for episodic memory) and even for decision-making if needed. The architecture is flexible – for instance, a **transformer with a longer context window** could be used as an *episodic memory reader*, directly attending over a batch of retrieved memory vectors along with the current state. Given compute limits, we likely wouldn't attend over millions of memories directly, but a hierarchical attention (first attend to summaries, etc.) could be used.

**Tool use and distributed memory:** If multiple agents or distributed instances need to share memory, we can integrate the memory as an external tool or service accessible via the LLM. For example, an agent's LLM might have a "tool" called `QuerySemanticDB(query)` or `SearchEpisodes(query)` that it can call (via a plugin or API) and get results, which it then incorporates into its reasoning. This modular approach keeps memory queries efficient (the heavy lifting done by a database) and allows multiple agents to query a common knowledge server. In a distributed agent scenario, we might host the episodic memory vector index on a server node; all agent processes send queries to it and get back results. As long as the network latency is low (<100ms), this works. We can also partition memory by agent or by content type (one server holds semantic facts, another holds episodes) and replicate as needed for scalability.

The LLM can also be fine-tuned or instructed to handle the **formatting of memory** – for example, generating citations or identifiers for memories it wants. One can imagine the agent asking itself (via chain-of-thought): "Do I recall anything about X?" and then triggering a retrieval. This tight integration means the agent's "mind" can fluidly move between what it *knows (LLM)* and what it *remembers (external memory)*.

Finally, we ensure that using an LLM doesn't violate the 100ms decision bound by **caching** and **optimization**. Complex reasoning might take longer, but routine queries (like retrieving a known fact) can be answered by a quick vector lookup or a short LLM completion. We can cache frequently needed knowledge in working memory or a short-term cache to avoid redundant LLM calls (for example, once the agent looks up the recipe for a torch, it can keep it in working memory while working on it).

## Example: Memory Lifecycle in Learning a Complex Skill

**Example Scenario:** *"Building a Defensive Tower"* – an agent in a sandbox RPG game learns how to construct a defensive watchtower over multiple days.

1. **Initial Attempts (Working & Episodic Memory):** The agent receives a quest to build a watchtower. Initially, it has no stored procedure for this. The goal and current environment (materials available, terrain) are loaded into **working memory**. The agent plans step-by-step using the LLM, and improvises: gather wood, build a platform, etc. Each attempt and its outcome are recorded into **episodic memory**. The first day, the tower collapses due to weak base. This failure episode is stored with high surprise/importance (structure collapse is novel). The agent notes in working memory "tower collapsed – need stronger base," which is then saved as part of the episodic record.

2. **Reflection and Consolidation (Semantic Memory):** That night (in a rest period), the agent's consolidation process kicks in. It retrieves the day's episodes related to building structures. The LLM is prompted: *"Summarize what was learned about building stable structures."* From the collapse event, it infers a semantic rule: **"A tall structure needs a wider base for stability."** This rule is stored in **semantic memory** (general knowledge). The agent also flags the successful parts (e.g. "the ladder to climb worked fine") and might store a sub-skill "building a ladder" in procedural memory. Unnecessary details (like how many nails were used exactly) are dropped to save space.

3. **Next Attempts (Episodic + Semantic Retrieval):** The next day, the agent tries again. This time, before building, it queries its memory. Working memory triggers retrieval: *relevant semantic memory:* "wider base for stability" is pulled up; *relevant episodes:* memory of yesterday's collapse is retrieved (with details like "tower height 10m, base 1m – collapsed"). These are loaded into working memory. With this guidance, the agent modifies its plan (makes the base 3m wide). The build succeeds. This successful sequence of actions (gather wood, make 3m base, add platform, add roof) is stored as a new **episodic entry**. Importantly, because it achieved the goal, this episode is marked with high importance (quest success).

4. **Skill Formation (Procedural Memory):** After a few iterations refining the tower design, the agent has a stable blueprint. During the next consolidation cycle, it decides to **form a procedural memory**: "BuildWatchtower" skill. It creates a template plan (or script) with steps and parameter slots (height, materials). This skill is stored in procedural memory with an embedding of keywords ("build", "tower", "defense"). Additionally, the general principle "wider base is better" remains in semantic memory for other construction tasks.

5. **Future Use (Procedural Retrieval & Adaptation):** A week later, the agent is tasked with building a *guard tower* in a different town. Now, when planning, the agent's working memory query identifies the **BuildWatchtower** skill as relevant via procedural memory lookup (the description "guard tower" matches "watchtower" closely). The skill is retrieved in <100ms and loaded. The agent might adapt the parameters (perhaps this tower needs to be stone instead of wood, so it tweaks materials but keeps the structure). The result is a quick execution of the skill, succeeding on the first try – a huge improvement over the initial trial-and-error. This new instance is stored as another episode (to record that stone variant works too). The agent also updates semantic memory if needed (e.g. "stone towers can be taller due to material strength").

6. **Analogical Transfer:** Later, the agent faces a different but related challenge: build a *lighthouse* by the sea. It recalls the watchtower experiences by analogy (both are tall, stable structures). Semantic memory provides the principle of wide base; procedural memory offers the watchtower skill. The agent adapts it (maybe adding a light at the top) and successfully builds a lighthouse – even though it never explicitly learned "how to build a lighthouse," its memory of structurally similar constructions allowed it to transfer knowledge. This demonstrates analogical reasoning in action, powered by the multi-type memory system.

Throughout this lifecycle, note how each memory type played a role:

* WM held the immediate goals and pulled relevant info in each context.
* EM accumulated the trials (failures and successes) as concrete experiences.
* SM captured the general rule (wider base) that applied beyond one tower.
* PM consolidated the successful method into a callable skill.
* If another agent had helped or if this were a team quest, **Social Memory** would record who contributed which idea, so if a similar project comes up the agent might re-engage the same helper or recall that "Bob" is an expert in stone building.

This example shows the interplay and **lifecycle**: raw experience → episodic memory → semantic insight → procedural skill → future retrieval → improved performance. Over a long time, the agent's behavior becomes more **knowledge-driven** (using memory) rather than improvised. Importantly, this happens under the hood in a scalable way: the agent doesn't store every tower brick position, only the meaningful abstractions, which is why it can scale to many experiences.

## Evaluation Benchmarks and Tasks

To validate such a memory system, we would evaluate it on a variety of **lifelong learning tasks** and compare performance with and without the advanced memory. Key benchmark scenarios include:

* **Long-Horizon Quest Completion:** Open-world games like *Minecraft* or *Zelda: Breath of the Wild* style quests that require many steps over hours of gameplay. For example, the "Obtain Diamond" task in Minecraft (as used by M2PA) requires collecting materials and crafting tools in sequence. We measure success rate and learning curve over repeated quest attempts. A memory-enabled agent should show improving success (as it remembers what failed or succeeded previously) and handle interruptions (if quest spans multiple sessions). Metrics: quest success rate vs number of attempts, time taken, etc.

* **Continual Task Learning:** A sequence of tasks of increasing difficulty to test lifelong learning (e.g. a curriculum of crafting recipes or puzzles). The **LifelongAgentBench** proposes evaluating agents on a strict sequence of tasks to see if knowledge gained in early tasks is reused later. We would use a similar setup: the agent faces tasks A, B, C in order. Without memory, each might be learned from scratch; with memory, agent should solve B and C faster by recalling A. Metrics: reduction in exploration steps or mistakes on later tasks due to memory.

* **Surprise/Novelty Adaptation Tasks:** Environments that throw curveballs – e.g. sudden changes in game rules or rare events. For instance, in a survival game, once in a while an earthquake happens (rare event). We evaluate if the agent remembers the appropriate response when an earthquake strikes again much later. Memory should allow retention of these rare but important experiences. Metric: agent's performance on *second occurrence* of a rare event (should be significantly better than the first, showing memory of the first).

* **Social Interaction and Multi-Agent Scenarios:** If the game has NPCs or other learning agents, we test scenarios like **forming alliances, trade negotiations, or team quests**. For example, an agent meets several NPCs with different personalities and later has to persuade one of them for help – this tests if it recalls that NPC's preferences from past dialogue. Another benchmark could be the **Diplomacy** board game or a MMORPG-like setting where remembering others' past actions is key to strategy. Metrics: success in negotiations or social goals, as well as human-likeness of interactions (does the agent behave consistently, e.g. not forgetting a friend or betrayal).

* **Memory-based Puzzle Solving:** Tasks designed to explicitly require memory, such as a dungeon where clues are given in different rooms far apart, or a code is given early and must be used later. The **TextWorld** or interactive fiction games often require remembering hints from earlier in the story. We could evaluate the percentage of puzzles solved that involve long delays or detours (where a memory-less agent would fail unless it has infinite context). This stresses the retrieval mechanism's ability to bring back relevant info at the right time.

* **Analogical Reasoning Challenges:** Present the agent with a new problem that is not identical to past ones but structurally similar. For instance, if the agent learned to use a key to open a locked door, now give it a locked treasure chest with a different key – see if it transfers the concept of "use key on lock". There are AI benchmarks like the *ARC (Abstract Reasoning Corpus)* or Bongard problems that test pattern generalization; we can adapt simpler versions in-game (like build structure X analogous to known structure Y). Measure success rate on first-time encounters of these analogical tasks.

* **Real-Time Performance Metrics:** Because of the <100ms query requirement, we should also measure the memory query latency and its impact on frame-rate or decision frequency. For instance, in a busy environment (like multiple agents all querying memory), does the retrieval still stay within budget? We could stress test with, say, 10 agents simultaneously performing and querying memory, ensuring the system scales (this tests the distributed aspect). Metric: average and 95th-percentile query time, and game simulation speed.

For each of these, we would compare our system against baselines: (a) an agent with no long-term memory (only short context), (b) perhaps an agent with a simpler replay buffer but no structured semantic/procedural memory. We expect to see improvements such as: **faster learning**, **higher ultimate performance**, **better adaptation to changes**, and **more coherent, human-like behavior** (especially in social contexts). For instance, the memory-enhanced agent should show a non-zero shot success on tasks that it failed initially after it has relevant experiences stored (demonstrating *learning transfer*), whereas a memory-less one resets each time.

Another evaluation approach is qualitative: examine the agent's behavior logs to see if it indeed uses its memories in reasoning. In the Generative Agents paper, they show agents recalling past events to influence their current dialog plans – we could similarly log the content of retrieved memories and ensure they are influencing decisions (e.g. the agent says "I remember the last time this happened…" or internally chooses a strategy because it recalls the outcome).

Finally, we consider **forgetting vs remembering**: we might test that the agent doesn't forget important things over a long duration. For example, after 100 in-game days, does it still recall the critical fact from day 1's tutorial? Ideally, consolidation ensured that if it was important, it's now in semantic memory. This could be tested by pop quizzes or randomly asking the agent questions about earlier events.

## Comparison with Biological Memory Systems

Our memory system design takes heavy inspiration from human and animal memory systems, and it's instructive to compare them:

* **Separation of Memory Types:** In human cognition, psychologists distinguish working memory (short-term scratchpad in prefrontal cortex), episodic memory (experiences linked to specific times/places, dependent on the hippocampus), semantic memory (general knowledge stored across the cortex), and procedural memory (skills/habits in basal ganglia and cerebellum). Our architecture explicitly mirrors this division. This helps ensure we cover the diverse functionality needed (from moment-to-moment reasoning to lifelong skill acquisition). We even added **social memory**, which in humans might not be a separate category per se, but humans do have specialized circuitry for recognizing individuals and remembering social information (e.g. the fusiform face area for faces, theory-of-mind networks for tracking others' mental states). In our agent, social memory is a distinct module for clarity, but functionally it's a blend of episodic (memory of interactions) and semantic (knowledge about a person).

* **Working Memory Limits:** Humans can hold ~7±2 items in working memory (according to Miller's classic result). Our agent similarly doesn't overload working memory – it only holds a small set of relevant facts at once. We implement a form of **chunking** (like humans group information) by retrieving summarized memories instead of raw logs. Also, the concept of a **focus of attention** is present: the agent's working memory corresponds to what it is concentrating on right now (just as humans can't think of everything at once, neither can the agent). By having to actively retrieve info, we ensure the agent's "mind" is not cluttered with irrelevant details, which parallels selective attention in humans.

* **Episodic to Semantic Consolidation:** Neurologically, the hippocampus encodes recent episodes and over time "teaches" the cortex, leading to stable semantic memories (this is why patients with hippocampal damage can recall old facts but not form new ones). Our system's consolidation process is an analog: episodic events gradually turn into semantic rules or knowledge. We even mimic nighttime replay – the agent "replays" or analyzes experiences offline much like hippocampal replay during sleep is believed to strengthen memories. This makes our agent more biologically plausible and also practically effective in not overfilling episodic storage.

* **Procedural Learning:** Humans often start learning a task with explicit reasoning (using working/episodic memory, e.g. following a recipe), but with practice it becomes implicit (you just do it without thinking of each step – that's procedural memory). We emulate this by turning frequently used action sequences into cached skills in procedural memory, so the agent no longer needs to deliberate on each low-level step. For instance, learning to ride a bike requires attention and episodic learning at first, but eventually becomes procedural and one can do it "without thinking." Our agent similarly would first solve tasks via reasoning + trial and error (slow), and later have a one-shot skill (fast) for it. One difference: humans sometimes find it hard to verbalize their procedural knowledge ("knowing how" vs "knowing that"), whereas our agent's skills could still be inspected or described since we store them as code or explicit policies.

* **Forgetting and Memory Updating:** Human memory is fallible – we forget details, and our memories can merge or distort. Our agent's forgetting is more intentional (dropping low-value data), but one could see parallels in how unaccessed memories weaken. Also, the agent can *update* its semantic memory when facts change (like relearning something) – humans also update beliefs, though sometimes with difficulty. One advantage of the agent: it can literally delete or edit a memory entry, whereas human memory doesn't have a direct API and thus old info can linger as misconceptions. We might consider adding a mechanism for memory "trace decay" or error correction to mirror how humans gradually adjust memories.

* **Analogical reasoning:** Humans are extremely good at analogy, often subconsciously. We rely on schemas learned from life (e.g. "story of David and Goliath" as a schema for underdog wins, applied to many domains). Our agent tries to replicate this by storing schemas and doing reasoning over them. However, current AI might not be as fluid unless guided – hence using the LLM to explicitly attempt analogies. In cognitive terms, we're giving the agent an ability like *reflective reasoning* which humans have when they deliberately draw parallels.

* **Social cognition:** Humans maintain rich social memories – we remember individuals' personalities, past interactions ("Alice lent me money, so I trust her"). Our social memory module is relatively simple (facts and episodes about agents), but it's a step toward that. In the Sims-like environment of Generative Agents, such a memory enabled believable social behavior. The agent could recall who did what and form opinions (we could even simulate "emotional tagging" of memories to denote positive or negative experiences with someone, akin to human emotion affecting memory strength).

**Key Differences/Challenges:** Unlike the brain, our system is not fully parallel – memory retrieval is a distinct operation, whereas the brain seems to retrieve associatively with cues in a massively parallel fashion. We approximate that with fast vector search, but it's still a designed process. Also, capacity-wise, humans have an enormous but not unlimited memory store, and our 1GB limit might be much smaller relatively. However, by efficient compression, the agent's 1GB could encode quite a lot (since it's high-level info, not raw sensory stream). Another difference is memory reliability: we can choose to have perfectly recallable memory (bits don't randomly fade in a computer unless we code it so), while human memory is noisy. We might intentionally add some noise or stochasticity to retrieval to mimic that sometimes not everything comes to mind – though from a performance view, we prefer important things always be accessible if needed.

In summary, the proposed memory system stands as a **broad and deep integration** of memory inspired by biology but tailored to digital agent constraints. It is broad in covering multiple memory types and functions (from momentary context to lifelong knowledge, from solitary skills to social knowledge), and it goes deep where it matters – e.g. using advanced retrieval algorithms to ensure the agent truly benefits from its massive experience, and using consolidation to keep the knowledge usable. This design positions a lifelong learning agent to be **adaptive, knowledgeable, and efficient** in open-world environments, continually learning from the world while keeping past wisdom at its fingertips.

**Sources:** The design builds upon cognitive architectures and recent agent research, including human memory theory, cognitive agent frameworks, and empirical findings in LLM-based agents (e.g. generative agents using combined recency/relevance memory retrieval and hybrid memory systems for open-world gameplay). These informed our approach to balancing different memory needs in an integrated system.